{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac0c89d",
   "metadata": {},
   "source": [
    "# CD4_P1_FT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb11524",
   "metadata": {},
   "source": [
    "### Variable Paths, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a756063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'CD4_P1_FT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13c43b",
   "metadata": {},
   "source": [
    "I use 3 Local Machines & 2 Cloud Compute Engines (Google, Kaggle). The appropriate paths for each platform of execution are declared here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0920ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle = 0\n",
    "Colab = 0\n",
    "Local = 1\n",
    "LocalRM = 0\n",
    "LocalOldLaptop = 0\n",
    "\n",
    "# Kaggle Notebooks\n",
    "Kaggle_Dataset = '/kaggle/input/catsdogsconv/KaggleCatsDogsConv'\n",
    "Kaggle_SavedModels = '/kaggle/working/SavedModels'\n",
    "Kaggle_TrainingHistory = '/kaggle/working/TrainingHistory'\n",
    "\n",
    "# Google Drive\n",
    "GD_Dataset = '/content/drive/MyDrive/Datasets/KaggleCatsDogs'\n",
    "GD_SavedModels = '/content/drive/MyDrive/NotebookWorkspace/SavedModels'\n",
    "CD_TrainingHistory = '/content/drive/MyDrive/NotebookWorkspace/TrainingHistory'\n",
    "\n",
    "# Local Directories\n",
    "Lc_Dataset = 'C:\\\\Programming_Files\\\\JupyterVSCode\\\\Binary_Classification_Transfer_Learning\\\\CatsDogs\\\\DatasetConv'\n",
    "Lc_SavedModels = 'C:\\\\Programming_Files\\\\JupyterVSCode\\\\Binary_Classification_Transfer_Learning\\\\CatsDogs\\\\SavedModels'\n",
    "Lc_TrainingHistory = 'C:\\\\Programming_Files\\\\JupyterVSCode\\\\Binary_Classification_Transfer_Learning\\\\CatsDogs\\\\Docs_Reports\\\\RawTrainingData'\n",
    "\n",
    "Lc_RM_Dataset = \"C:\\\\Users\\\\arisi\\\\Documents\\\\VSCode\\\\CatsDogs\\\\Dataset\\\\KaggleCatsDogsConv\"\n",
    "Lc_RM_SavedModels = \"C:\\\\Users\\\\arisi\\\\Documents\\\\VSCode\\\\CatsDogs\\\\SavedModels\"\n",
    "Lc_RM_TrainingHistory = \"C:\\\\Users\\\\arisi\\\\Documents\\\\VSCode\\\\CatsDogs\\\\Docs_Reports\\\\RawTrainingData\"\n",
    "\n",
    "Lc_Old_Dataset = 'Test'\n",
    "Lc_Old_SavedModels = 'Test'\n",
    "Lc_Old_TrainingHistory = 'Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Kaggle:\n",
    "    data_dir = Kaggle_Dataset\n",
    "    SavedModelsPath = Kaggle_SavedModels\n",
    "    TrainingHistoryPath = Kaggle_TrainingHistory\n",
    "if Colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    data_dir = GD_Dataset\n",
    "    SavedModelsPath = GD_SavedModels\n",
    "    TrainingHistoryPath = CD_TrainingHistory\n",
    "if Local:\n",
    "    data_dir = Lc_Dataset\n",
    "    SavedModelsPath = Lc_SavedModels\n",
    "    TrainingHistoryPath = Lc_TrainingHistory\n",
    "if LocalRM:\n",
    "    data_dir = Lc_RM_Dataset\n",
    "    SavedModelsPath = Lc_RM_SavedModels\n",
    "    TrainingHistoryPath = Lc_RM_TrainingHistory\n",
    "if LocalOldLaptop:\n",
    "    data_dir = Lc_Old_Dataset\n",
    "    SavedModelsPath = Lc_Old_SavedModels\n",
    "    TrainingHistoryPath = Lc_Old_TrainingHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c2830",
   "metadata": {},
   "source": [
    "### CD 2 Model, P1 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bffc94",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ab191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2,  # 20% for validation\n",
    "    subset='training',     # Use the 'training' subset\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.2,  # 20% for validation\n",
    "    subset='validation',   # Use the 'validation' subset\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c39f7",
   "metadata": {},
   "source": [
    "### Preprocessing & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation layer\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.1),  # 10% random rotation\n",
    "    layers.RandomZoom(0.1),      # 10% zoom\n",
    "    layers.RandomTranslation(0.1, 0.1),  # Random height and width shift\n",
    "    layers.RandomBrightness(0.2)\n",
    "])\n",
    "\n",
    "# Augment the training data\n",
    "def augment_img(image, label):\n",
    "    image = data_augmentation(image)  # Apply augmentations\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(augment_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a94b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VGG-16 preprocessing\n",
    "def preprocess_img(image, label):\n",
    "    image = preprocess_input(image)  # Apply VGG16-specific preprocessing\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_img)\n",
    "val_dataset = val_dataset.map(preprocess_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b3676",
   "metadata": {},
   "source": [
    "### Load Model with Trained Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c988f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = ''\n",
    "model = tf.keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc8d77",
   "metadata": {},
   "source": [
    "### Unfreeze Last VGG Block & Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last few layers (e.g., last 4 layers)\n",
    "for layer in model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Optionally, print trainable status\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i}: {layer.name}, Trainable: {layer.trainable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[0].layers:\n",
    "    if layer.name in ['block5_conv1', 'block5_conv2', 'block5_conv3']:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45400541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_layers(model, indent=0):\n",
    "    for layer in model.layers:\n",
    "        print(\" \" * indent + f\"- {layer.name} ({layer.__class__.__name__}), Trainable: {layer.trainable}\")\n",
    "        # If this layer has sublayers (like Functional or Sequential models)\n",
    "        if hasattr(layer, 'layers'):\n",
    "            print_model_layers(layer, indent + 2)\n",
    "\n",
    "print_model_layers(model)\n",
    "\n",
    "print(model.optimizer.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834969d5",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f'{modelname}'\n",
    "checkpoint_path = f\"{SavedModelsPath}\\\\{name}_{{epoch:03d}}_val{{val_loss:.4f}}.keras\"\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_freq='epoch',              # Save every epoch\n",
    "    save_weights_only=False,\n",
    "    save_best_only=False,           # Save every time, not just best\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3289a9f",
   "metadata": {},
   "source": [
    "### Train & Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7967fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=6,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d6a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "name = f'{modelname}'\n",
    "filepath = f\"{TrainingHistoryPath}\\\\{name}.json\"\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb6eb",
   "metadata": {},
   "source": [
    "### Continue Training (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint_callback, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2964d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "name = f'{modelname}_continue'\n",
    "filepath = f\"{TrainingHistoryPath}\\\\{name}.json\"\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(history2.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvPy3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
