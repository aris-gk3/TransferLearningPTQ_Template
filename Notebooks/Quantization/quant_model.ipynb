{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d4dc0d",
   "metadata": {},
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22cec5a",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a05e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ml_project_util.path import path_definition # type: error\n",
    "from ml_project_util.flatten_model import flatten_condtitional\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3515d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Packages\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.flatten_model import flatten_condtitional\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise\n",
    "from ml_project_util.quantization_util import quant_weights, quant_activations, quant_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586875b4",
   "metadata": {},
   "source": [
    "### Variable Paths, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cef0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH, PATH_DATASET, PATH_RAWDATA, PATH_JOINEDDATA, PATH_SAVEDMODELS = path_definition()\n",
    "model_name = 'CD4_P2_FT_003_val0.0336'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b6ec4",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef6ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = model_name[:3]\n",
    "filepath = f'{PATH_SAVEDMODELS}/{parent_folder}/{model_name}.keras'\n",
    "model = tf.keras.models.load_model(filepath)\n",
    "model = flatten_condtitional(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a542550",
   "metadata": {},
   "source": [
    "### Quantize & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a61344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Quantization range not found in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json, recalculating.\n",
      "Read activation range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_range.json\n",
      "Read weight range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n",
      "Read weight scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_scale.json\n",
      "Read activation sw scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_scale.json\n",
      "Read complete json dictionary from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_complete_dict.json\n",
      "Quantization range could not be saved in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json!\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98620\n",
      "Precise val loss: 0.04226\n"
     ]
    }
   ],
   "source": [
    "qw_model = quant_weights(model, model_name, num_bits=8, mode='quant')\n",
    "qwa_model =  quant_activations(model, model_name, num_bits=8, mode='eval', design='hw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85b0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Quantization range not found in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json, recalculating.\n",
      "Read activation range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_range.json\n",
      "Read weight range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n",
      "Read weight scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_scale.json\n",
      "Read activation sw scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_scale.json\n",
      "Read complete json dictionary from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_complete_dict.json\n",
      "Quantization range could not be saved in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json!\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.97500\n",
      "Precise val loss: 0.06892\n"
     ]
    }
   ],
   "source": [
    "qw_model = quant_weights(model, model_name, num_bits=7, mode='quant')\n",
    "qwa_model =  quant_activations(model, model_name, num_bits=7, mode_func='eval', mode='hw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89e783a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Quantization range not found in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json, recalculating.\n",
      "Read activation range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_range.json\n",
      "Read weight range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n",
      "Read weight scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_scale.json\n",
      "Read activation sw scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_scale.json\n",
      "Read complete json dictionary from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_complete_dict.json\n",
      "Quantization range could not be saved in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json!\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.53191\n",
      "Precise val loss: 0.75527\n"
     ]
    }
   ],
   "source": [
    "qw_model = quant_weights(model, model_name, num_bits=6, mode='quant')\n",
    "qwa_model =  quant_activations(model, model_name, num_bits=6, mode_func='eval', mode='hw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff3d95",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b799843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Quantization range not found in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json, recalculating.\n",
      "Read activation range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_range.json\n",
      "Read weight range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n",
      "Read weight scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_scale.json\n",
      "Read activation sw scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_scale.json\n",
      "Read complete json dictionary from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_complete_dict.json\n",
      "Quantization range saved in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json!\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98860\n",
      "Precise val loss: 0.03685\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m qwa_model = quant_model(model, model_name, num_bits=\u001b[32m9\u001b[39m, design=\u001b[33m'\u001b[39m\u001b[33mhw\u001b[39m\u001b[33m'\u001b[39m, batch_len=\u001b[32m157\u001b[39m, force=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\ml_project_util\\quantization_util.py:1459\u001b[39m, in \u001b[36mquant_model\u001b[39m\u001b[34m(model, model_name, num_bits, design, batch_len, force)\u001b[39m\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInvalid input.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(save==\u001b[32m1\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmp_filepath, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1460\u001b[39m         metric_dict = json.load(f)\n\u001b[32m   1461\u001b[39m     metric_dict[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_bits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m] = {\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: acc, \u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: loss}\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json'"
     ]
    }
   ],
   "source": [
    "qwa_model = quant_model(model, model_name, num_bits=9, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011003de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json\n",
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98860\n",
      "Precise val loss: 0.03728\n"
     ]
    }
   ],
   "source": [
    "qwa_model = quant_model(model, model_name, num_bits=9, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03406\n"
     ]
    }
   ],
   "source": [
    "# custom\n",
    "qwa_model = quant_model(model, model_name, num_bits=32, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a20c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom\n",
    "qwa_model = quant_model(model, model_name, num_bits=31, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e67e86",
   "metadata": {},
   "source": [
    "### Test for quantization of biases (layer output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae51cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json\n",
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Precise val accuracy: 1.00000\n",
      "Precise val loss: 0.00109\n"
     ]
    }
   ],
   "source": [
    "# custom\n",
    "qwa_model = quant_model(model, model_name, num_bits=8, design='hw', batch_len=2, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff5cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json\n",
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Precise val accuracy: 1.00000\n",
      "Precise val loss: 0.00122\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=8, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d42f6",
   "metadata": {},
   "source": [
    "### New Runs (for bias precision=layer output & custom quant layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4dc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json\n",
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98440\n",
      "Precise val loss: 0.05470\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=7, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8ed844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read input range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Metrics/CD4_P2_FT_003_metrics_quant_hw.json\n",
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98720\n",
      "Precise val loss: 0.03842\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=8, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "412579f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98760\n",
      "Precise val loss: 0.03725\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=9, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3471ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98800\n",
      "Precise val loss: 0.03577\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=10, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bbb277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98860\n",
      "Precise val loss: 0.03417\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=11, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d213af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03390\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=12, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066f883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03388\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=13, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d74df6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d98015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98900\n",
      "Precise val loss: 0.03389\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=14, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edcaf8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=15, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c7ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=16, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74ff21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5424f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=17, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ea18986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=18, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cde9586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=19, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1e0ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce91fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=20, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e1ba6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=21, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b775e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=22, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fd9ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c54a04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=23, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "078a4271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=24, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eec4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=25, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef1d94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e371d522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=26, design='hw', batch_len=157, force=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1314081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=27, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9209f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=28, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27076d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bff2b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=29, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23c8729f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=30, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e153598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b91492a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=31, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df9acfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98920\n",
      "Precise val loss: 0.03386\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output\n",
    "qwa_model = quant_model(model, model_name, num_bits=32, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16776e6",
   "metadata": {},
   "source": [
    "### New Runs (2) + floor operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7bc81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "hw activation quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98540\n",
      "Precise val loss: 0.04337\n"
     ]
    }
   ],
   "source": [
    "# custom quantization layer, bias based on layer output, floor operation for bit shifting\n",
    "qwa_model = quant_model(model, model_name, num_bits=8, design='hw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701bd86",
   "metadata": {},
   "source": [
    "### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e484636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No accuracy per bitwidth json file found in specified path!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# save accuracy in json in\n",
    "bitwidth = 8\n",
    "mode = \"qw\"\n",
    "short_name = model_name[:-10]\n",
    "accuracy_path = f'{BASE_PATH}/Docs_Reports/Quant/{short_name}_acc_bw_{mode}.json'\n",
    "# read json\n",
    "try:\n",
    "    with open(accuracy_path, 'r') as file:\n",
    "        acc_bw_dict = json.load(file)\n",
    "except:\n",
    "    print('No accuracy per bitwidth json file found in specified path!')\n",
    "    acc_bw_dict = {}\n",
    "# add in dict\n",
    "acc_bw_dict[f'{bitwidth}b'] = { \"accuracy\": 0.97840, \"loss\": 0.11877}\n",
    "# write json\n",
    "with open(accuracy_path, 'w') as f:\n",
    "    json.dump(acc_bw_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be15e90",
   "metadata": {},
   "source": [
    "### Save qw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79496c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same path + qw subfolder\n",
    "parent_folder = model_name[:3]\n",
    "short_name = model_name[:-10]\n",
    "model_path = f'{PATH_SAVEDMODELS}/{parent_folder}/Quant/{short_name}_qw.keras'\n",
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvPy3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
