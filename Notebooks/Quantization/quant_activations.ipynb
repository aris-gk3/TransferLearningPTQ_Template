{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "997d32db",
   "metadata": {},
   "source": [
    "# Activation Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec78ca",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4350aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras import layers, Sequential # type: error\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input # type: error\n",
    "from tensorflow.keras.layers import Conv2D, Dense # type: error\n",
    "from tensorflow.keras.models import clone_model # type: error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b60178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Packages\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.flatten_model import flatten_condtitional\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise\n",
    "from ml_project_util.quantization_util import quant_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303b451",
   "metadata": {},
   "source": [
    "### Variable Paths, Names, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e757a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH, PATH_DATASET, PATH_RAWDATA, PATH_JOINEDDATA, PATH_SAVEDMODELS = path_definition()\n",
    "model_name = 'CD4_P2_FT_003_val0.0336'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d6625",
   "metadata": {},
   "source": [
    "### Load Float Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb335a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,056,261</span> (114.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,056,261\u001b[0m (114.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,473,665</span> (28.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,473,665\u001b[0m (28.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,635,264</span> (29.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,635,264\u001b[0m (29.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,947,332</span> (57.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m14,947,332\u001b[0m (57.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "short_name = model_name[:-10]\n",
    "parent_name = model_name[:3]\n",
    "filepath = f'{PATH_SAVEDMODELS}/{parent_name}/{model_name}.keras'\n",
    "model = tf.keras.models.load_model(filepath)\n",
    "model = flatten_condtitional(model, model_name)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20da84e",
   "metadata": {},
   "source": [
    "### Create New Model with Fake Quant Layers & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3378b7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Quantization range not found in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json, recalculating.\n",
      "Read activation range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_range.json\n",
      "input_layer: min = -151.0610, max = 151.0610\n",
      "block1_conv1: min = 0.0000, max = 932.9594\n",
      "block1_conv2: min = 0.0000, max = 3747.8220\n",
      "block2_conv1: min = 0.0000, max = 7530.5459\n",
      "block2_conv2: min = 0.0000, max = 12263.2021\n",
      "block3_conv1: min = 0.0000, max = 18402.4238\n",
      "block3_conv2: min = 0.0000, max = 18064.2148\n",
      "block3_conv3: min = 0.0000, max = 21941.4180\n",
      "block4_conv1: min = 0.0000, max = 14641.2715\n",
      "block4_conv2: min = 0.0000, max = 6983.7476\n",
      "block4_conv3: min = 0.0000, max = 4462.6309\n",
      "block5_conv1: min = 0.0000, max = 2829.6855\n",
      "block5_conv2: min = 0.0000, max = 1204.6249\n",
      "block5_conv3: min = 0.0000, max = 527.8722\n",
      "dense: min = 0.0000, max = 29.6658\n",
      "dense_1: min = 0.0000, max = 28.7891\n",
      "dense_2: min = 0.0000, max = 1.0000\n",
      "Read weight range json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n",
      "{\n",
      "  \"block1_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.6714000701904297,\n",
      "      \"max\": 0.6085159182548523\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.015828926116228104,\n",
      "      \"max\": 2.0640370845794678\n",
      "    }\n",
      "  },\n",
      "  \"block1_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.21561293303966522,\n",
      "      \"max\": 0.2891709506511688\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -1.027151346206665,\n",
      "      \"max\": 0.9052184224128723\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.33594822883605957,\n",
      "      \"max\": 0.41661107540130615\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.17922063171863556,\n",
      "      \"max\": 0.36547425389289856\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1819043755531311,\n",
      "      \"max\": 0.277375727891922\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5953347682952881,\n",
      "      \"max\": 0.6337577700614929\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.183063343167305,\n",
      "      \"max\": 0.5444108247756958\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.20097896456718445,\n",
      "      \"max\": 0.34949612617492676\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17710502445697784,\n",
      "      \"max\": 0.45931634306907654\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1812487542629242,\n",
      "      \"max\": 0.2748450040817261\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17968426644802094,\n",
      "      \"max\": 0.3915373682975769\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1428879201412201,\n",
      "      \"max\": 0.5947717428207397\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12409957498311996,\n",
      "      \"max\": 0.3138822615146637\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.14548234641551971,\n",
      "      \"max\": 0.31484508514404297\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.10524698346853256,\n",
      "      \"max\": 0.337660014629364\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.08428452908992767,\n",
      "      \"max\": 0.18237581849098206\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12739665806293488,\n",
      "      \"max\": 0.2562357187271118\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.19835947453975677,\n",
      "      \"max\": 0.33766546845436096\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11336661875247955,\n",
      "      \"max\": 0.18989436328411102\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.3510940968990326,\n",
      "      \"max\": 0.6397964358329773\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1370379626750946,\n",
      "      \"max\": 0.20495359599590302\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.9170470237731934,\n",
      "      \"max\": 0.7601493000984192\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.09244729578495026,\n",
      "      \"max\": 0.2867557108402252\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5003161430358887,\n",
      "      \"max\": 9.431466102600098\n",
      "    }\n",
      "  },\n",
      "  \"dense\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11981535702943802,\n",
      "      \"max\": 0.11917836219072342\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.019688373431563377,\n",
      "      \"max\": 0.012206909246742725\n",
      "    }\n",
      "  },\n",
      "  \"dense_1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1130584329366684,\n",
      "      \"max\": 0.11739388853311539\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.031707119196653366,\n",
      "      \"max\": 0.022229688242077827\n",
      "    }\n",
      "  },\n",
      "  \"dense_2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.13492853939533234,\n",
      "      \"max\": 0.1367005556821823\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.01994166150689125,\n",
      "      \"max\": -0.01994166150689125\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Read weight scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_scale.json\n",
      "block1_conv1: scale = 0.00528661\n",
      "block1_conv2: scale = 0.00227694\n",
      "block2_conv1: scale = 0.00328040\n",
      "block2_conv2: scale = 0.00218406\n",
      "block3_conv1: scale = 0.00428670\n",
      "block3_conv2: scale = 0.00361666\n",
      "block3_conv3: scale = 0.00308297\n",
      "block4_conv1: scale = 0.00247151\n",
      "block4_conv2: scale = 0.00265874\n",
      "block4_conv3: scale = 0.00201760\n",
      "block5_conv1: scale = 0.00149523\n",
      "block5_conv2: scale = 0.00161381\n",
      "block5_conv3: scale = 0.00225792\n",
      "dense: scale = 0.00094343\n",
      "dense_1: scale = 0.00092436\n",
      "dense_2: scale = 0.00107638\n",
      "Read activation sw scale json from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_sw_scale.json\n",
      "input_layer: scale = 1.18945673\n",
      "block1_conv1: scale = 7.34613710\n",
      "block1_conv2: scale = 29.51040962\n",
      "block2_conv1: scale = 59.29563700\n",
      "block2_conv2: scale = 96.56064684\n",
      "block3_conv1: scale = 144.90097502\n",
      "block3_conv2: scale = 142.23791216\n",
      "block3_conv3: scale = 172.76707062\n",
      "block4_conv1: scale = 115.28560224\n",
      "block4_conv2: scale = 54.99013826\n",
      "block4_conv3: scale = 35.13882566\n",
      "block5_conv1: scale = 22.28098856\n",
      "block5_conv2: scale = 9.48523526\n",
      "block5_conv3: scale = 4.15647396\n",
      "dense: scale = 0.23358937\n",
      "dense_1: scale = 0.22668568\n",
      "dense_2: scale = 0.00787402\n",
      "Read complete json dictionary from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_complete_dict.json\n",
      "{\n",
      "    \"activation_hw_scale\": {\n",
      "        \"input_layer\": 1.1894567294383611,\n",
      "        \"block1_conv1\": 12.878232497521196,\n",
      "        \"block1_conv2\": 30.026669225443623,\n",
      "        \"block2_conv1\": 100.86354006057421,\n",
      "        \"block2_conv2\": 112.78955980420155,\n",
      "        \"block3_conv1\": 247.54940887319395,\n",
      "        \"block3_conv2\": 229.19758455442556,\n",
      "        \"block3_conv3\": 180.892057351259,\n",
      "        \"block4_conv1\": 114.45177056041004,\n",
      "        \"block4_conv2\": 77.90016810690453,\n",
      "        \"block4_conv3\": 40.2359545223771,\n",
      "        \"block5_conv1\": 30.802972079995957,\n",
      "        \"block5_conv2\": 12.725779946142382,\n",
      "        \"block5_conv3\": 7.355847709132279,\n",
      "        \"dense\": 0.44414161613800707,\n",
      "        \"dense_1\": 0.4204004696920525,\n",
      "        \"dense_2\": 0.01448037236307503\n",
      "    },\n",
      "    \"activation_sw_scale\": {\n",
      "        \"input_layer\": 1.1894567294383611,\n",
      "        \"block1_conv1\": 7.346137099378691,\n",
      "        \"block1_conv2\": 29.510409617987204,\n",
      "        \"block2_conv1\": 59.29563699557087,\n",
      "        \"block2_conv2\": 96.56064683809055,\n",
      "        \"block3_conv1\": 144.9009750246063,\n",
      "        \"block3_conv2\": 142.23791215551182,\n",
      "        \"block3_conv3\": 172.76707062007873,\n",
      "        \"block4_conv1\": 115.28560223917323,\n",
      "        \"block4_conv2\": 54.990138256643704,\n",
      "        \"block4_conv3\": 35.13882566437008,\n",
      "        \"block5_conv1\": 22.280988558070867,\n",
      "        \"block5_conv2\": 9.485235259288878,\n",
      "        \"block5_conv3\": 4.156473955770177,\n",
      "        \"dense\": 0.23358936760369248,\n",
      "        \"dense_1\": 0.2266856816809947,\n",
      "        \"dense_2\": 0.007874015748031496\n",
      "    },\n",
      "    \"wt_scale\": {\n",
      "        \"block1_conv1\": 0.005286614725908895,\n",
      "        \"block1_conv2\": 0.002276936619300542,\n",
      "        \"block2_conv1\": 0.0032804021685142216,\n",
      "        \"block2_conv2\": 0.002184060849542693,\n",
      "        \"block3_conv1\": 0.004286699407682644,\n",
      "        \"block3_conv2\": 0.003616664118654146,\n",
      "        \"block3_conv3\": 0.0030829714039179285,\n",
      "        \"block4_conv1\": 0.002471513870194202,\n",
      "        \"block4_conv2\": 0.0026587402726721576,\n",
      "        \"block4_conv3\": 0.0020176040844654473,\n",
      "        \"block5_conv1\": 0.0014952312069615042,\n",
      "        \"block5_conv2\": 0.0016138078424874254,\n",
      "        \"block5_conv3\": 0.002257918982993899,\n",
      "        \"dense\": 0.0009434280081058112,\n",
      "        \"dense_1\": 0.0009243613270324046,\n",
      "        \"dense_2\": 0.00107638232820616\n",
      "    },\n",
      "    \"activation_hw_range_dict\": {\n",
      "        \"input_layer\": {\n",
      "            \"min\": -151.06100463867188,\n",
      "            \"max\": 151.06100463867188\n",
      "        },\n",
      "        \"block1_conv1\": {\n",
      "            \"min\": -1635.535527185192,\n",
      "            \"max\": 1635.535527185192\n",
      "        },\n",
      "        \"block1_conv2\": {\n",
      "            \"min\": -3813.38699163134,\n",
      "            \"max\": 3813.38699163134\n",
      "        },\n",
      "        \"block2_conv1\": {\n",
      "            \"min\": -12809.669587692924,\n",
      "            \"max\": 12809.669587692924\n",
      "        },\n",
      "        \"block2_conv2\": {\n",
      "            \"min\": -14324.274095133596,\n",
      "            \"max\": 14324.274095133596\n",
      "        },\n",
      "        \"block3_conv1\": {\n",
      "            \"min\": -31438.774926895632,\n",
      "            \"max\": 31438.774926895632\n",
      "        },\n",
      "        \"block3_conv2\": {\n",
      "            \"min\": -29108.093238412046,\n",
      "            \"max\": 29108.093238412046\n",
      "        },\n",
      "        \"block3_conv3\": {\n",
      "            \"min\": -22973.291283609895,\n",
      "            \"max\": 22973.291283609895\n",
      "        },\n",
      "        \"block4_conv1\": {\n",
      "            \"min\": -14535.374861172075,\n",
      "            \"max\": 14535.374861172075\n",
      "        },\n",
      "        \"block4_conv2\": {\n",
      "            \"min\": -9893.321349576876,\n",
      "            \"max\": 9893.321349576876\n",
      "        },\n",
      "        \"block4_conv3\": {\n",
      "            \"min\": -5109.966224341892,\n",
      "            \"max\": 5109.966224341892\n",
      "        },\n",
      "        \"block5_conv1\": {\n",
      "            \"min\": -3911.9774541594866,\n",
      "            \"max\": 3911.9774541594866\n",
      "        },\n",
      "        \"block5_conv2\": {\n",
      "            \"min\": -1616.1740531600824,\n",
      "            \"max\": 1616.1740531600824\n",
      "        },\n",
      "        \"block5_conv3\": {\n",
      "            \"min\": -934.1926590597994,\n",
      "            \"max\": 934.1926590597994\n",
      "        },\n",
      "        \"dense\": {\n",
      "            \"min\": -56.4059852495269,\n",
      "            \"max\": 56.4059852495269\n",
      "        },\n",
      "        \"dense_1\": {\n",
      "            \"min\": -53.39085965089067,\n",
      "            \"max\": 53.39085965089067\n",
      "        },\n",
      "        \"dense_2\": {\n",
      "            \"min\": -1.8390072901105288,\n",
      "            \"max\": 1.8390072901105288\n",
      "        }\n",
      "    },\n",
      "    \"activation_sw_range_dict\": {\n",
      "        \"input_layer\": {\n",
      "            \"min\": -151.06100463867188,\n",
      "            \"max\": 151.06100463867188\n",
      "        },\n",
      "        \"block1_conv1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 932.9594116210938\n",
      "        },\n",
      "        \"block1_conv2\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 3747.822021484375\n",
      "        },\n",
      "        \"block2_conv1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 7530.5458984375\n",
      "        },\n",
      "        \"block2_conv2\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 12263.2021484375\n",
      "        },\n",
      "        \"block3_conv1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 18402.423828125\n",
      "        },\n",
      "        \"block3_conv2\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 18064.21484375\n",
      "        },\n",
      "        \"block3_conv3\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 21941.41796875\n",
      "        },\n",
      "        \"block4_conv1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 14641.271484375\n",
      "        },\n",
      "        \"block4_conv2\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 6983.74755859375\n",
      "        },\n",
      "        \"block4_conv3\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 4462.630859375\n",
      "        },\n",
      "        \"block5_conv1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 2829.685546875\n",
      "        },\n",
      "        \"block5_conv2\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 1204.6248779296875\n",
      "        },\n",
      "        \"block5_conv3\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 527.8721923828125\n",
      "        },\n",
      "        \"dense\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 29.665849685668945\n",
      "        },\n",
      "        \"dense_1\": {\n",
      "            \"min\": 0.0,\n",
      "            \"max\": 28.789081573486328\n",
      "        },\n",
      "        \"dense_2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1.0\n",
      "        }\n",
      "    },\n",
      "    \"wt_range\": {\n",
      "        \"block1_conv1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.6714000701904297,\n",
      "                \"max\": 0.6085159182548523\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.015828926116228104,\n",
      "                \"max\": 2.0640370845794678\n",
      "            }\n",
      "        },\n",
      "        \"block1_conv2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.21561293303966522,\n",
      "                \"max\": 0.2891709506511688\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -1.027151346206665,\n",
      "                \"max\": 0.9052184224128723\n",
      "            }\n",
      "        },\n",
      "        \"block2_conv1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.33594822883605957,\n",
      "                \"max\": 0.41661107540130615\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.17922063171863556,\n",
      "                \"max\": 0.36547425389289856\n",
      "            }\n",
      "        },\n",
      "        \"block2_conv2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.1819043755531311,\n",
      "                \"max\": 0.277375727891922\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.5953347682952881,\n",
      "                \"max\": 0.6337577700614929\n",
      "            }\n",
      "        },\n",
      "        \"block3_conv1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.183063343167305,\n",
      "                \"max\": 0.5444108247756958\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.20097896456718445,\n",
      "                \"max\": 0.34949612617492676\n",
      "            }\n",
      "        },\n",
      "        \"block3_conv2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.17710502445697784,\n",
      "                \"max\": 0.45931634306907654\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.1812487542629242,\n",
      "                \"max\": 0.2748450040817261\n",
      "            }\n",
      "        },\n",
      "        \"block3_conv3\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.17968426644802094,\n",
      "                \"max\": 0.3915373682975769\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.1428879201412201,\n",
      "                \"max\": 0.5947717428207397\n",
      "            }\n",
      "        },\n",
      "        \"block4_conv1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.12409957498311996,\n",
      "                \"max\": 0.3138822615146637\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.14548234641551971,\n",
      "                \"max\": 0.31484508514404297\n",
      "            }\n",
      "        },\n",
      "        \"block4_conv2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.10524698346853256,\n",
      "                \"max\": 0.337660014629364\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.08428452908992767,\n",
      "                \"max\": 0.18237581849098206\n",
      "            }\n",
      "        },\n",
      "        \"block4_conv3\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.12739665806293488,\n",
      "                \"max\": 0.2562357187271118\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.19835947453975677,\n",
      "                \"max\": 0.33766546845436096\n",
      "            }\n",
      "        },\n",
      "        \"block5_conv1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.11336661875247955,\n",
      "                \"max\": 0.18989436328411102\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.3510940968990326,\n",
      "                \"max\": 0.6397964358329773\n",
      "            }\n",
      "        },\n",
      "        \"block5_conv2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.1370379626750946,\n",
      "                \"max\": 0.20495359599590302\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.9170470237731934,\n",
      "                \"max\": 0.7601493000984192\n",
      "            }\n",
      "        },\n",
      "        \"block5_conv3\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.09244729578495026,\n",
      "                \"max\": 0.2867557108402252\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.5003161430358887,\n",
      "                \"max\": 9.431466102600098\n",
      "            }\n",
      "        },\n",
      "        \"dense\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.11981535702943802,\n",
      "                \"max\": 0.11917836219072342\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.019688373431563377,\n",
      "                \"max\": 0.012206909246742725\n",
      "            }\n",
      "        },\n",
      "        \"dense_1\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.1130584329366684,\n",
      "                \"max\": 0.11739388853311539\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.031707119196653366,\n",
      "                \"max\": 0.022229688242077827\n",
      "            }\n",
      "        },\n",
      "        \"dense_2\": {\n",
      "            \"weight\": {\n",
      "                \"min\": -0.13492853939533234,\n",
      "                \"max\": 0.1367005556821823\n",
      "            },\n",
      "            \"bias\": {\n",
      "                \"min\": -0.01994166150689125,\n",
      "                \"max\": -0.01994166150689125\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"shift\": {\n",
      "        \"input_layer\": 0,\n",
      "        \"block1_conv1\": 11,\n",
      "        \"block1_conv2\": 10,\n",
      "        \"block2_conv1\": 10,\n",
      "        \"block2_conv2\": 9,\n",
      "        \"block3_conv1\": 9,\n",
      "        \"block3_conv2\": 8,\n",
      "        \"block3_conv3\": 8,\n",
      "        \"block4_conv1\": 8,\n",
      "        \"block4_conv2\": 8,\n",
      "        \"block4_conv3\": 8,\n",
      "        \"block5_conv1\": 9,\n",
      "        \"block5_conv2\": 8,\n",
      "        \"block5_conv3\": 8,\n",
      "        \"dense\": 6,\n",
      "        \"dense_1\": 10,\n",
      "        \"dense_2\": 5\n",
      "    }\n",
      "}\n",
      "Quantization range could not be saved in C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_activation_hw_range.json!\n",
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98780\n",
      "Precise val loss: 0.03924\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model =  quant_activations(model, model_name, num_bits=8, mode_func='eval', mode='hw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_precise(quant_aware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e748f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1336c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float range dictionary path\n",
    "range_name = model_name[:-10]\n",
    "range_dict_path = f'{BASE_PATH}/Docs_Reports/Quant/Ranges/{range_name}_activation_range.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2713ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense\n",
    "\n",
    "# Custom FakeQuantLayer simulates quantization but preserves shape\n",
    "class FakeQuantLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, min_val=-6.0, max_val=6.0):\n",
    "        super().__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.quantization.fake_quant_with_min_max_vars(inputs, min=self.min_val, max=self.max_val)\n",
    "    \n",
    "class SymmetricFakeQuantLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_abs_val=6.0, num_bits=8, narrow_range=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_abs_val = max_abs_val\n",
    "        self.min_val = -max_abs_val\n",
    "        self.max_val = max_abs_val\n",
    "        self.num_bits = num_bits\n",
    "        self.narrow_range = narrow_range  # Set to True for signed int8 [-127, 127]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.quantization.fake_quant_with_min_max_vars(\n",
    "            inputs,\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "            num_bits=self.num_bits,\n",
    "            narrow_range=self.narrow_range\n",
    "        )\n",
    "\n",
    "def clone_model_with_fake_quant(original_model, input_shape, range_dict):\n",
    "    new_model = Sequential()\n",
    "    layer_mapping = []\n",
    "    quant_layers_list = list(range_dict.keys())\n",
    "\n",
    "    # Add input layer explicitly\n",
    "    new_model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    quant_layer = 0\n",
    "    for layer in original_model.layers:\n",
    "        config = layer.get_config()\n",
    "        cloned_layer = layer.__class__.from_config(config)\n",
    "        # Insert fake quant after Conv2D or Dense\n",
    "        if isinstance(cloned_layer, (Conv2D, Dense)):\n",
    "            tmp_min = range_dict[quant_layers_list[quant_layer]]['min']\n",
    "            tmp_max = range_dict[quant_layers_list[quant_layer]]['max']\n",
    "            abs_max = abs(tmp_min) if abs(tmp_min)>tmp_max else tmp_max\n",
    "            #new_model.add(FakeQuantLayer(min_val=tmp_min, max_val=tmp_max))\n",
    "            new_model.add(SymmetricFakeQuantLayer(max_abs_val=abs_max))\n",
    "            quant_layer = quant_layer + 1\n",
    "        # Clone layer from config\n",
    "        new_model.add(cloned_layer)\n",
    "        layer_mapping.append((layer, cloned_layer))\n",
    "\n",
    "    # Build model by running dummy data through it\n",
    "    dummy_input = tf.random.uniform((1, *input_shape))\n",
    "    new_model(dummy_input)\n",
    "\n",
    "    # Copy weights from original layers to cloned layers\n",
    "    for orig_layer, cloned_layer in layer_mapping:\n",
    "        if orig_layer.weights and cloned_layer.weights:\n",
    "            try:\n",
    "                cloned_layer.set_weights(orig_layer.get_weights())\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping weights for layer {orig_layer.name} due to mismatch: {e}\")\n",
    "\n",
    "    new_model.build(input_shape=(None, *input_shape))  # Step 2\n",
    "\n",
    "    dummy_input = tf.random.uniform((1, *input_shape))  # Step 3\n",
    "    new_model(dummy_input)\n",
    "\n",
    "    print(\"New model input shape:\", new_model.input_shape)  # Step 4\n",
    "\n",
    "    for orig_layer, cloned_layer in layer_mapping:\n",
    "        try:\n",
    "            cloned_layer.set_weights(orig_layer.get_weights())\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping weights for {orig_layer.name}: {e}\")\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48da5420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "New model input shape: (None, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ symmetric_fake_quant_layer      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_2    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_6    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_7    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_8    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_9    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_10   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_11   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_12   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_13   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_14   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_15   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ symmetric_fake_quant_layer      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_2    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_6    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_7    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_8    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_9    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_10   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_11   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_12   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_13   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_14   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_15   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,108,929</span> (57.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,108,929\u001b[0m (57.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,473,665</span> (28.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,473,665\u001b[0m (28.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,635,264</span> (29.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,635,264\u001b[0m (29.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "try:\n",
    "    with open(range_dict_path, 'r') as file:\n",
    "        range_dict = json.load(file)\n",
    "except:\n",
    "    print('No float range dictionary found!')\n",
    "quant_aware_model = clone_model_with_fake_quant(model, input_shape, range_dict)\n",
    "quant_aware_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869dd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98980\n",
      "Precise val loss: 0.03686\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_precise(quant_aware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7f19d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8e5f4",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(range_dict_path, 'r') as file:\n",
    "        range_dict = json.load(file)\n",
    "except:\n",
    "    print('No float range dictionary found!')\n",
    "quant_layers_list = list(range_dict.keys())\n",
    "quant_layers_list\n",
    "tmp_max = range_dict[quant_layers_list[i]]['max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b91110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedInput(tf.keras.layers.Layer):\n",
    "    def __init__(self, min_val=-1.0, max_val=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.quantization.fake_quant_with_min_max_vars(\n",
    "            inputs, min=self.min_val, max=self.max_val, num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_quant_layer(x, min_val=-6.0, max_val=6.0):\n",
    "    return tf.quantization.fake_quant_with_min_max_vars(x, min=min_val, max=max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77efa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "for layer in model.layers:\n",
    "    cloned_layer = tf.keras.models.clone_model(layer)\n",
    "    cloned_layer.build(layer.input_shape)\n",
    "    cloned_layer.set_weights(layer.get_weights())\n",
    "    new_model.add(cloned_layer)\n",
    "\n",
    "    if isinstance(layer, (tf.keras.layers.Conv2D, tf.keras.layers.Dense)):\n",
    "        new_model.add(QuantizedInput())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d61c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create new model\n",
    "# new_model = Sequential()\n",
    "\n",
    "# # Insert layers from the old model one by one\n",
    "# i = 0\n",
    "# for layer in model.layers:\n",
    "#     # Insert new layer after the first Dense\n",
    "#     if isinstance(layer, Conv2D) or isinstance(layer, Dense):\n",
    "#         tmp_max = range_dict[quant_layers_list[i]]['max']\n",
    "#         new_model.add(QuantizedInput(min_val=0, max_val=tmp_max))\n",
    "#         new_model.add(layer)\n",
    "#         i = i + 1\n",
    "\n",
    "# if i != len(quant_layers_list):\n",
    "#     print(\"Wrong layer handling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f3aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model_layers_list = []\n",
    "# for i in new_model.layers:\n",
    "#     new_model_layers_list.append(i.name)\n",
    "\n",
    "# i = 0\n",
    "# for layer in model.layers:\n",
    "#     if layer == new_model_layers_list[i]:\n",
    "#         print(f\"Copying {layer.name} weights!\")\n",
    "#         new_layer = new_model.layers[new_model_layers_list[i]]\n",
    "#         new_layer.set_weights(layer.get_weights())\n",
    "#         i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d3897",
   "metadata": {},
   "source": [
    "### Evaluate new model with quantized activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvPy3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
