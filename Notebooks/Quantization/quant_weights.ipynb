{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb3df93",
   "metadata": {},
   "source": [
    "# Weights Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b53085",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0420b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Local packages\n",
    "from ml_project_util.path import path_definition # type: error\n",
    "from ml_project_util.flatten_model import flatten_condtitional # type: error\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise # type: error\n",
    "from ml_project_util.quantization_util import wt_range_search, quant_weights # type: error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcab22c",
   "metadata": {},
   "source": [
    "### Variable Paths, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2f55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH, PATH_DATASET, PATH_RAWDATA, PATH_JOINEDDATA, PATH_SAVEDMODELS = path_definition()\n",
    "model_name = 'CD4_P2_FT_003_val0.0336'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771c8f9",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bc1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name = model_name[:-10]\n",
    "parent_name = model_name[:3]\n",
    "filepath = f'{PATH_SAVEDMODELS}/{parent_name}/{model_name}.keras'\n",
    "model = tf.keras.models.load_model(filepath)\n",
    "model = flatten_condtitional(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b585f7b",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2689563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98940\n",
      "Precise val loss: 0.03345\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_precise(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98b4bc7",
   "metadata": {},
   "source": [
    "### Quantize Weights and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b60a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization range has been read from C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json.\n",
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98900\n",
      "Precise val loss: 0.03448\n"
     ]
    }
   ],
   "source": [
    "qw_model = quant_weights(model, model_name, num_bits=8, mode='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3b3fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_project_util.quantization_util import quantize_tensor_symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8ac8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_path='0'\n",
    "quant='symmetric'\n",
    "mode='eval'\n",
    "batch_len=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25d5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight quantization not found in , searching now...\n",
      "{\n",
      "  \"block1_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.6714000701904297,\n",
      "      \"max\": 0.6085159182548523\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.015828926116228104,\n",
      "      \"max\": 2.0640370845794678\n",
      "    }\n",
      "  },\n",
      "  \"block1_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.21561293303966522,\n",
      "      \"max\": 0.2891709506511688\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -1.027151346206665,\n",
      "      \"max\": 0.9052184224128723\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.33594822883605957,\n",
      "      \"max\": 0.41661107540130615\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.17922063171863556,\n",
      "      \"max\": 0.36547425389289856\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1819043755531311,\n",
      "      \"max\": 0.277375727891922\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5953347682952881,\n",
      "      \"max\": 0.6337577700614929\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.183063343167305,\n",
      "      \"max\": 0.5444108247756958\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.20097896456718445,\n",
      "      \"max\": 0.34949612617492676\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17710502445697784,\n",
      "      \"max\": 0.45931634306907654\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1812487542629242,\n",
      "      \"max\": 0.2748450040817261\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17968426644802094,\n",
      "      \"max\": 0.3915373682975769\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1428879201412201,\n",
      "      \"max\": 0.5947717428207397\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12409957498311996,\n",
      "      \"max\": 0.3138822615146637\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.14548234641551971,\n",
      "      \"max\": 0.31484508514404297\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.10524698346853256,\n",
      "      \"max\": 0.337660014629364\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.08428452908992767,\n",
      "      \"max\": 0.18237581849098206\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12739665806293488,\n",
      "      \"max\": 0.2562357187271118\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.19835947453975677,\n",
      "      \"max\": 0.33766546845436096\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11336661875247955,\n",
      "      \"max\": 0.18989436328411102\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.3510940968990326,\n",
      "      \"max\": 0.6397964358329773\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1370379626750946,\n",
      "      \"max\": 0.20495359599590302\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.9170470237731934,\n",
      "      \"max\": 0.7601493000984192\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.09244729578495026,\n",
      "      \"max\": 0.2867557108402252\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5003161430358887,\n",
      "      \"max\": 9.431466102600098\n",
      "    }\n",
      "  },\n",
      "  \"dense\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11981535702943802,\n",
      "      \"max\": 0.11917836219072342\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.019688373431563377,\n",
      "      \"max\": 0.012206909246742725\n",
      "    }\n",
      "  },\n",
      "  \"dense_1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1130584329366684,\n",
      "      \"max\": 0.11739388853311539\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.031707119196653366,\n",
      "      \"max\": 0.022229688242077827\n",
      "    }\n",
      "  },\n",
      "  \"dense_2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.13492853939533234,\n",
      "      \"max\": 0.1367005556821823\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.01994166150689125,\n",
      "      \"max\": -0.01994166150689125\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Saved json in: C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n"
     ]
    }
   ],
   "source": [
    "if(range_path=='0'):\n",
    "    BASE_PATH, _, _, _, _ = path_definition()\n",
    "    short_name = model_name[:-10]\n",
    "    filepath = f'{BASE_PATH}/Docs_Reports/Quant/Ranges/{short_name}_wt_range.json'\n",
    "    filepath = f''\n",
    "else:\n",
    "    filepath = range_path\n",
    "try:\n",
    "    with open(filepath, 'r') as f:\n",
    "        range_dict = json.load(f)\n",
    "    print(f'Weight quantization range has been read from {filepath}.')\n",
    "except:\n",
    "    print(f'Weight quantization not found in {filepath}, searching now...')\n",
    "    weight_ranges = wt_range_search(model, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178d0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6714000701904297\n",
      "[array([[[[ 4.29470569e-01,  1.17273867e-01,  3.40129584e-02, ...,\n",
      "          -1.32241577e-01, -5.33475243e-02,  7.57738389e-03],\n",
      "         [ 5.50379455e-01,  2.08774377e-02,  9.88311544e-02, ...,\n",
      "          -8.48205537e-02, -5.11389151e-02,  3.74943428e-02],\n",
      "         [ 4.80015397e-01, -1.72696680e-01,  3.75577137e-02, ...,\n",
      "          -1.27135560e-01, -5.02991639e-02,  3.48965675e-02]],\n",
      "\n",
      "        [[ 3.73466998e-01,  1.62062630e-01,  1.70863140e-03, ...,\n",
      "          -1.48207128e-01, -2.35300660e-01, -6.30356818e-02],\n",
      "         [ 4.40074533e-01,  4.73412387e-02,  5.13819456e-02, ...,\n",
      "          -9.88498852e-02, -2.96195745e-01, -7.04357103e-02],\n",
      "         [ 4.08547401e-01, -1.70375049e-01, -4.96297423e-03, ...,\n",
      "          -1.22360572e-01, -2.76450396e-01, -3.90796512e-02]],\n",
      "\n",
      "        [[-6.13601133e-02,  1.35693997e-01, -1.15694344e-01, ...,\n",
      "          -1.40158370e-01, -3.77666801e-01, -3.00509870e-01],\n",
      "         [-8.13870355e-02,  4.18543853e-02, -1.01763301e-01, ...,\n",
      "          -9.43124294e-02, -5.05662560e-01, -3.83694321e-01],\n",
      "         [-6.51455522e-02, -1.54351532e-01, -1.38038069e-01, ...,\n",
      "          -1.29404560e-01, -4.62243795e-01, -3.23985279e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 2.74769872e-01,  1.48350164e-01,  1.61559835e-01, ...,\n",
      "          -1.14316158e-01,  3.65494519e-01,  3.39938998e-01],\n",
      "         [ 3.45739067e-01,  3.10493708e-02,  2.40750551e-01, ...,\n",
      "          -6.93419054e-02,  4.37116861e-01,  4.13171440e-01],\n",
      "         [ 3.10477257e-01, -1.87601492e-01,  1.66595340e-01, ...,\n",
      "          -9.88388434e-02,  4.04058546e-01,  3.92561197e-01]],\n",
      "\n",
      "        [[ 3.86807770e-02,  2.02298447e-01,  1.56414255e-01, ...,\n",
      "          -5.20089604e-02,  2.57149011e-01,  3.71682674e-01],\n",
      "         [ 4.06322069e-02,  6.58102185e-02,  2.20311403e-01, ...,\n",
      "          -3.78979952e-03,  2.69412428e-01,  4.09505904e-01],\n",
      "         [ 5.02023660e-02, -1.77571565e-01,  1.51188180e-01, ...,\n",
      "          -1.40649760e-02,  2.59300828e-01,  4.23764467e-01]],\n",
      "\n",
      "        [[-3.67223352e-01,  1.61688417e-01, -8.99365395e-02, ...,\n",
      "          -1.45945460e-01, -2.71823555e-01, -2.39718184e-01],\n",
      "         [-4.53501314e-01,  4.62574959e-02, -6.67438358e-02, ...,\n",
      "          -1.03502415e-01, -3.45792353e-01, -2.92486250e-01],\n",
      "         [-4.03383434e-01, -1.74399972e-01, -1.09849639e-01, ...,\n",
      "          -1.25688612e-01, -3.14026326e-01, -2.32839763e-01]]],\n",
      "\n",
      "\n",
      "       [[[-5.74681684e-02,  1.29344285e-01,  1.29030216e-02, ...,\n",
      "          -1.41449392e-01,  2.41099641e-01,  4.55602147e-02],\n",
      "         [-5.86349145e-02,  3.16787697e-02,  7.59588331e-02, ...,\n",
      "          -1.05017252e-01,  3.39550197e-01,  9.86374393e-02],\n",
      "         [-5.08716851e-02, -1.66002661e-01,  1.56279504e-02, ...,\n",
      "          -1.49742723e-01,  3.06801915e-01,  8.82701725e-02]],\n",
      "\n",
      "        [[-2.62249678e-01,  1.71572417e-01,  5.44555223e-05, ...,\n",
      "          -1.22728683e-01,  2.44687453e-01,  5.32913655e-02],\n",
      "         [-3.30669671e-01,  5.47101051e-02,  4.86797579e-02, ...,\n",
      "          -8.29023942e-02,  2.95466095e-01,  7.44469985e-02],\n",
      "         [-2.85227507e-01, -1.66666731e-01, -7.96697661e-03, ...,\n",
      "          -1.09780088e-01,  2.79203743e-01,  9.46525261e-02]],\n",
      "\n",
      "        [[-3.50096762e-01,  1.38710454e-01, -1.25339806e-01, ...,\n",
      "          -1.53092295e-01, -1.39917329e-01, -2.65075237e-01],\n",
      "         [-4.85030204e-01,  4.23195846e-02, -1.12076312e-01, ...,\n",
      "          -1.18306056e-01, -1.67058021e-01, -3.22241962e-01],\n",
      "         [-4.18516338e-01, -1.57048807e-01, -1.49133086e-01, ...,\n",
      "          -1.56839803e-01, -1.42874300e-01, -2.69694626e-01]]]],\n",
      "      dtype=float32), array([ 0.73429835,  0.09340367,  0.06775674,  0.8862966 ,  0.25994542,\n",
      "        0.66426694, -0.01582893,  0.3249065 ,  0.68600726,  0.06247932,\n",
      "        0.58156496,  0.2361475 ,  0.69694996,  0.19451167,  0.4858922 ,\n",
      "        0.44571847,  0.5113422 ,  0.208576  ,  0.57557714,  0.33199573,\n",
      "        0.4997983 ,  0.7117759 ,  0.30284074,  0.7082712 ,  0.04548979,\n",
      "        0.7446502 ,  0.29845494,  0.48211655,  0.81658626,  0.62603897,\n",
      "        0.3768093 ,  2.064037  ,  0.77311045,  0.3459577 ,  0.6130958 ,\n",
      "        0.65459156,  0.39045632,  0.50869167,  0.2625384 ,  0.23669638,\n",
      "        0.07971057,  1.1179353 ,  0.26129362,  0.8697589 ,  0.21543622,\n",
      "        0.78007823,  0.37015367,  0.47993386,  0.4313978 ,  0.5084194 ,\n",
      "        0.23049663,  0.7636527 ,  0.35419866,  0.45794216,  0.4662595 ,\n",
      "        0.09850298,  0.3803252 ,  0.66880196,  0.4015123 ,  0.90510356,\n",
      "        0.43166816,  1.302014  ,  0.5306885 ,  0.48993504], dtype=float32)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m             \u001b[38;5;28mprint\u001b[39m(layer_ranges[\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m             \u001b[38;5;28mprint\u001b[39m(weights)\n\u001b[32m      9\u001b[39m             new_weights = [\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m                 quantize_tensor_symmetric(w, w_range)\n\u001b[32m     11\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m w, w_range \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(weights, layer_ranges)\n\u001b[32m     12\u001b[39m             ]\n\u001b[32m     13\u001b[39m             layer.set_weights(new_weights)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# evaluate new model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aris_Work\\anaconda3\\envs\\EnvPy3_12\\Lib\\site-packages\\ml_project_util\\quantization_util.py:935\u001b[39m, in \u001b[36mquantize_tensor_symmetric\u001b[39m\u001b[34m(w, w_range, num_bits)\u001b[39m\n\u001b[32m    932\u001b[39m qmin = -(\u001b[32m2\u001b[39m ** (num_bits - \u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# -127 for int8\u001b[39;00m\n\u001b[32m    933\u001b[39m qmax = (\u001b[32m2\u001b[39m ** (num_bits - \u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m)   \u001b[38;5;66;03m# +127 for int8\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m w_min = w_range[\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    936\u001b[39m w_max = w_range[\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Use symmetric range centered at 0\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "# Clone weights to new model\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"get_weights\") and hasattr(layer, \"set_weights\"):\n",
    "        weights = layer.get_weights()\n",
    "        if weights and layer.name in weight_ranges:\n",
    "            layer_ranges = weight_ranges[layer.name]['weight']\n",
    "            print(layer_ranges['min'])\n",
    "            print(weights)\n",
    "            new_weights = [\n",
    "                quantize_tensor_symmetric(w, w_range)\n",
    "                for w, w_range in zip(weights, layer_ranges)\n",
    "            ]\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "# evaluate new model\n",
    "if(mode=='eval'):\n",
    "    model_evaluation_precise(model, batch_len=batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08385db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"block1_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.6714000701904297,\n",
      "      \"max\": 0.6085159182548523\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.015828926116228104,\n",
      "      \"max\": 2.0640370845794678\n",
      "    }\n",
      "  },\n",
      "  \"block1_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.21561293303966522,\n",
      "      \"max\": 0.2891709506511688\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -1.027151346206665,\n",
      "      \"max\": 0.9052184224128723\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.33594822883605957,\n",
      "      \"max\": 0.41661107540130615\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.17922063171863556,\n",
      "      \"max\": 0.36547425389289856\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1819043755531311,\n",
      "      \"max\": 0.277375727891922\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5953347682952881,\n",
      "      \"max\": 0.6337577700614929\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.183063343167305,\n",
      "      \"max\": 0.5444108247756958\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.20097896456718445,\n",
      "      \"max\": 0.34949612617492676\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17710502445697784,\n",
      "      \"max\": 0.45931634306907654\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1812487542629242,\n",
      "      \"max\": 0.2748450040817261\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17968426644802094,\n",
      "      \"max\": 0.3915373682975769\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1428879201412201,\n",
      "      \"max\": 0.5947717428207397\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12409957498311996,\n",
      "      \"max\": 0.3138822615146637\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.14548234641551971,\n",
      "      \"max\": 0.31484508514404297\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.10524698346853256,\n",
      "      \"max\": 0.337660014629364\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.08428452908992767,\n",
      "      \"max\": 0.18237581849098206\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12739665806293488,\n",
      "      \"max\": 0.2562357187271118\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.19835947453975677,\n",
      "      \"max\": 0.33766546845436096\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11336661875247955,\n",
      "      \"max\": 0.18989436328411102\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.3510940968990326,\n",
      "      \"max\": 0.6397964358329773\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1370379626750946,\n",
      "      \"max\": 0.20495359599590302\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.9170470237731934,\n",
      "      \"max\": 0.7601493000984192\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.09244729578495026,\n",
      "      \"max\": 0.2867557108402252\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5003161430358887,\n",
      "      \"max\": 9.431466102600098\n",
      "    }\n",
      "  },\n",
      "  \"dense\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11981535702943802,\n",
      "      \"max\": 0.11917836219072342\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.019688373431563377,\n",
      "      \"max\": 0.012206909246742725\n",
      "    }\n",
      "  },\n",
      "  \"dense_1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1130584329366684,\n",
      "      \"max\": 0.11739388853311539\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.031707119196653366,\n",
      "      \"max\": 0.022229688242077827\n",
      "    }\n",
      "  },\n",
      "  \"dense_2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.13492853939533234,\n",
      "      \"max\": 0.1367005556821823\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.01994166150689125,\n",
      "      \"max\": -0.01994166150689125\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Saved json in: C:/Programming_Files/JupyterVSCode/Binary_Classification_Transfer_Learning/CatsDogs/Docs_Reports/Quant/Ranges/CD4_P2_FT_003_wt_range.json\n"
     ]
    }
   ],
   "source": [
    "weight_ranges = wt_range_search(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor_asymmetric_old(w, w_range, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = 2**num_bits - 1\n",
    "\n",
    "    w_min = w_range[\"min\"]\n",
    "    w_max = w_range[\"max\"]\n",
    "\n",
    "    # Avoid divide by zero\n",
    "    if w_max == w_min:\n",
    "        return w  # all weights are same\n",
    "\n",
    "    scale = (w_max - w_min) / (qmax - qmin)\n",
    "    zero_point = round(-w_min / scale)\n",
    "\n",
    "    # Quantize\n",
    "    q = np.round(w / scale + zero_point)\n",
    "    q = np.clip(q, qmin, qmax)\n",
    "\n",
    "    # Dequantize\n",
    "    w_dequant = (q - zero_point) * scale\n",
    "    return w_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae16892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor_symmetric(w, w_range, num_bits=8):\n",
    "    qmin = -(2 ** (num_bits - 1) - 1)  # -127 for int8\n",
    "    qmax = (2 ** (num_bits - 1) - 1)   # +127 for int8\n",
    "\n",
    "    w_min = w_range[\"min\"]\n",
    "    w_max = w_range[\"max\"]\n",
    "\n",
    "    # Use symmetric range centered at 0\n",
    "    max_abs = max(abs(w_min), abs(w_max))\n",
    "\n",
    "    if max_abs == 0:\n",
    "        return np.zeros_like(w)\n",
    "\n",
    "    scale = max_abs / qmax  # ensure 0 maps to 0, and max_abs maps to ±127\n",
    "\n",
    "    # Quantize\n",
    "    q = np.round(w / scale)\n",
    "    q = np.clip(q, qmin, qmax)\n",
    "\n",
    "    # Dequantize\n",
    "    w_dequant = q * scale\n",
    "\n",
    "    return w_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee680d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if hasattr(layer, \"get_weights\") and hasattr(layer, \"set_weights\"):\n",
    "        weights = layer.get_weights()\n",
    "        if weights and layer.name in weight_ranges:\n",
    "            layer_ranges = weight_ranges[layer.name]\n",
    "            new_weights = [\n",
    "                quantize_tensor_symmetric(w, w_range)\n",
    "                for w, w_range in zip(weights, layer_ranges)\n",
    "            ]\n",
    "            layer.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67d231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def quantize_weights(weights, num_bits=8):\n",
    "#     # Calculate quantization range\n",
    "#     qmin = -2**(num_bits - 1)\n",
    "#     qmax = 2**(num_bits - 1) - 1\n",
    "\n",
    "#     scale = np.max(np.abs(weights)) / qmax\n",
    "#     if scale == 0:\n",
    "#         return weights  # No quantization needed\n",
    "\n",
    "#     # Quantize and dequantize\n",
    "#     quantized = np.round(weights / scale)\n",
    "#     quantized = np.clip(quantized, qmin, qmax)\n",
    "#     dequantized = quantized * scale\n",
    "#     return dequantized\n",
    "\n",
    "# # Apply quantization layer by layer\n",
    "# for layer in model.layers:\n",
    "#     if hasattr(layer, \"get_weights\") and hasattr(layer, \"set_weights\"):\n",
    "#         weights = layer.get_weights()\n",
    "#         if weights:\n",
    "#             quantized_weights = [quantize_weights(w) for w in weights]\n",
    "#             layer.set_weights(quantized_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e4f425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98880\n",
      "Precise val loss: 0.03399\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_precise(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12633a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f71a9",
   "metadata": {},
   "source": [
    "### Create new model with fake quantization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212d3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float range dictionary path\n",
    "range_name = model_name[:-10]\n",
    "range_dict_path = f'{BASE_PATH}/Docs_Reports/Quant/Ranges/{range_name}_activation_range.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "358e39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense\n",
    "\n",
    "# Custom FakeQuantLayer simulates quantization but preserves shape\n",
    "class FakeQuantLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, min_val=-6.0, max_val=6.0):\n",
    "        super().__init__()\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.quantization.fake_quant_with_min_max_vars(inputs, min=self.min_val, max=self.max_val)\n",
    "    \n",
    "class SymmetricFakeQuantLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_abs_val=6.0, num_bits=8, narrow_range=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_abs_val = max_abs_val\n",
    "        self.min_val = -max_abs_val\n",
    "        self.max_val = max_abs_val\n",
    "        self.num_bits = num_bits\n",
    "        self.narrow_range = narrow_range  # Set to True for signed int8 [-127, 127]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.quantization.fake_quant_with_min_max_vars(\n",
    "            inputs,\n",
    "            min=self.min_val,\n",
    "            max=self.max_val,\n",
    "            num_bits=self.num_bits,\n",
    "            narrow_range=self.narrow_range\n",
    "        )\n",
    "\n",
    "def clone_model_with_fake_quant(original_model, input_shape, range_dict):\n",
    "    new_model = Sequential()\n",
    "    layer_mapping = []\n",
    "    quant_layers_list = list(range_dict.keys())\n",
    "\n",
    "    # Add input layer explicitly\n",
    "    new_model.add(tf.keras.Input(shape=input_shape))\n",
    "\n",
    "    quant_layer = 0\n",
    "    for layer in original_model.layers:\n",
    "        config = layer.get_config()\n",
    "        cloned_layer = layer.__class__.from_config(config)\n",
    "        # Insert fake quant after Conv2D or Dense\n",
    "        if isinstance(cloned_layer, (Conv2D, Dense)):\n",
    "            tmp_min = range_dict[quant_layers_list[quant_layer]]['min']\n",
    "            tmp_max = range_dict[quant_layers_list[quant_layer]]['max']\n",
    "            abs_max = abs(tmp_min) if abs(tmp_min)>tmp_max else tmp_max\n",
    "            #new_model.add(FakeQuantLayer(min_val=tmp_min, max_val=tmp_max))\n",
    "            new_model.add(SymmetricFakeQuantLayer(max_abs_val=abs_max))\n",
    "            quant_layer = quant_layer + 1\n",
    "        # Clone layer from config\n",
    "        new_model.add(cloned_layer)\n",
    "        layer_mapping.append((layer, cloned_layer))\n",
    "\n",
    "    # Build model by running dummy data through it\n",
    "    dummy_input = tf.random.uniform((1, *input_shape))\n",
    "    new_model(dummy_input)\n",
    "\n",
    "    # Copy weights from original layers to cloned layers\n",
    "    for orig_layer, cloned_layer in layer_mapping:\n",
    "        if orig_layer.weights and cloned_layer.weights:\n",
    "            try:\n",
    "                cloned_layer.set_weights(orig_layer.get_weights())\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping weights for layer {orig_layer.name} due to mismatch: {e}\")\n",
    "\n",
    "    new_model.build(input_shape=(None, *input_shape))  # Step 2\n",
    "\n",
    "    dummy_input = tf.random.uniform((1, *input_shape))  # Step 3\n",
    "    new_model(dummy_input)\n",
    "\n",
    "    print(\"New model input shape:\", new_model.input_shape)  # Step 4\n",
    "\n",
    "    for orig_layer, cloned_layer in layer_mapping:\n",
    "        try:\n",
    "            cloned_layer.set_weights(orig_layer.get_weights())\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping weights for {orig_layer.name}: {e}\")\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ccff933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model input shape: (None, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ symmetric_fake_quant_layer_16   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_17   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_18   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_19   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_20   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_21   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_22   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_23   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_24   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_25   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_26   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_27   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_28   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_29   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_30   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_31   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SymmetricFakeQuantLayer</span>)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ symmetric_fake_quant_layer_16   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_17   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_18   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_19   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_20   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_21   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_22   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_23   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_24   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_25   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_26   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_27   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_28   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_29   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_30   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ symmetric_fake_quant_layer_31   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSymmetricFakeQuantLayer\u001b[0m)       │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,108,929</span> (57.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,108,929\u001b[0m (57.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,473,665</span> (28.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,473,665\u001b[0m (28.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,635,264</span> (29.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,635,264\u001b[0m (29.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "try:\n",
    "    with open(range_dict_path, 'r') as file:\n",
    "        range_dict = json.load(file)\n",
    "except:\n",
    "    print('No float range dictionary found!')\n",
    "quant_aware_model = clone_model_with_fake_quant(model, input_shape, range_dict)\n",
    "quant_aware_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b65a6c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24997 files belonging to 2 classes.\n",
      "Using 4999 files for validation.\n",
      "Batch Number: 0\n",
      "Batch Number: 1\n",
      "Batch Number: 2\n",
      "Batch Number: 3\n",
      "Batch Number: 4\n",
      "Batch Number: 5\n",
      "Batch Number: 6\n",
      "Batch Number: 7\n",
      "Batch Number: 8\n",
      "Batch Number: 9\n",
      "Batch Number: 10\n",
      "Batch Number: 11\n",
      "Batch Number: 12\n",
      "Batch Number: 13\n",
      "Batch Number: 14\n",
      "Batch Number: 15\n",
      "Batch Number: 16\n",
      "Batch Number: 17\n",
      "Batch Number: 18\n",
      "Batch Number: 19\n",
      "Batch Number: 20\n",
      "Batch Number: 21\n",
      "Batch Number: 22\n",
      "Batch Number: 23\n",
      "Batch Number: 24\n",
      "Batch Number: 25\n",
      "Batch Number: 26\n",
      "Batch Number: 27\n",
      "Batch Number: 28\n",
      "Batch Number: 29\n",
      "Batch Number: 30\n",
      "Batch Number: 31\n",
      "Batch Number: 32\n",
      "Batch Number: 33\n",
      "Batch Number: 34\n",
      "Batch Number: 35\n",
      "Batch Number: 36\n",
      "Batch Number: 37\n",
      "Batch Number: 38\n",
      "Batch Number: 39\n",
      "Batch Number: 40\n",
      "Batch Number: 41\n",
      "Batch Number: 42\n",
      "Batch Number: 43\n",
      "Batch Number: 44\n",
      "Batch Number: 45\n",
      "Batch Number: 46\n",
      "Batch Number: 47\n",
      "Batch Number: 48\n",
      "Batch Number: 49\n",
      "Batch Number: 50\n",
      "Batch Number: 51\n",
      "Batch Number: 52\n",
      "Batch Number: 53\n",
      "Batch Number: 54\n",
      "Batch Number: 55\n",
      "Batch Number: 56\n",
      "Batch Number: 57\n",
      "Batch Number: 58\n",
      "Batch Number: 59\n",
      "Batch Number: 60\n",
      "Batch Number: 61\n",
      "Batch Number: 62\n",
      "Batch Number: 63\n",
      "Batch Number: 64\n",
      "Batch Number: 65\n",
      "Batch Number: 66\n",
      "Batch Number: 67\n",
      "Batch Number: 68\n",
      "Batch Number: 69\n",
      "Batch Number: 70\n",
      "Batch Number: 71\n",
      "Batch Number: 72\n",
      "Batch Number: 73\n",
      "Batch Number: 74\n",
      "Batch Number: 75\n",
      "Batch Number: 76\n",
      "Batch Number: 77\n",
      "Batch Number: 78\n",
      "Batch Number: 79\n",
      "Batch Number: 80\n",
      "Batch Number: 81\n",
      "Batch Number: 82\n",
      "Batch Number: 83\n",
      "Batch Number: 84\n",
      "Batch Number: 85\n",
      "Batch Number: 86\n",
      "Batch Number: 87\n",
      "Batch Number: 88\n",
      "Batch Number: 89\n",
      "Batch Number: 90\n",
      "Batch Number: 91\n",
      "Batch Number: 92\n",
      "Batch Number: 93\n",
      "Batch Number: 94\n",
      "Batch Number: 95\n",
      "Batch Number: 96\n",
      "Batch Number: 97\n",
      "Batch Number: 98\n",
      "Batch Number: 99\n",
      "Batch Number: 100\n",
      "Batch Number: 101\n",
      "Batch Number: 102\n",
      "Batch Number: 103\n",
      "Batch Number: 104\n",
      "Batch Number: 105\n",
      "Batch Number: 106\n",
      "Batch Number: 107\n",
      "Batch Number: 108\n",
      "Batch Number: 109\n",
      "Batch Number: 110\n",
      "Batch Number: 111\n",
      "Batch Number: 112\n",
      "Batch Number: 113\n",
      "Batch Number: 114\n",
      "Batch Number: 115\n",
      "Batch Number: 116\n",
      "Batch Number: 117\n",
      "Batch Number: 118\n",
      "Batch Number: 119\n",
      "Batch Number: 120\n",
      "Batch Number: 121\n",
      "Batch Number: 122\n",
      "Batch Number: 123\n",
      "Batch Number: 124\n",
      "Batch Number: 125\n",
      "Batch Number: 126\n",
      "Batch Number: 127\n",
      "Batch Number: 128\n",
      "Batch Number: 129\n",
      "Batch Number: 130\n",
      "Batch Number: 131\n",
      "Batch Number: 132\n",
      "Batch Number: 133\n",
      "Batch Number: 134\n",
      "Batch Number: 135\n",
      "Batch Number: 136\n",
      "Batch Number: 137\n",
      "Batch Number: 138\n",
      "Batch Number: 139\n",
      "Batch Number: 140\n",
      "Batch Number: 141\n",
      "Batch Number: 142\n",
      "Batch Number: 143\n",
      "Batch Number: 144\n",
      "Batch Number: 145\n",
      "Batch Number: 146\n",
      "Batch Number: 147\n",
      "Batch Number: 148\n",
      "Batch Number: 149\n",
      "Batch Number: 150\n",
      "Batch Number: 151\n",
      "Batch Number: 152\n",
      "Batch Number: 153\n",
      "Batch Number: 154\n",
      "Batch Number: 155\n",
      "Batch Number: 156\n",
      "Precise val accuracy: 0.98880\n",
      "Precise val loss: 0.03767\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_precise(quant_aware_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvPy3_12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
