{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a9a477",
   "metadata": {
    "_cell_guid": "0aaedd7b-7a24-4c41-80b7-8fb76b6b2c01",
    "_uuid": "67131900-a7e4-4b80-b2f2-4d8904a6ac99",
    "collapsed": false,
    "id": "bac0c89d",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003763,
     "end_time": "2025-09-23T09:48:11.132024",
     "exception": false,
     "start_time": "2025-09-23T09:48:11.128261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16288739",
   "metadata": {
    "_cell_guid": "3247ec71-b268-4779-a56b-fd22da32ee8f",
    "_uuid": "5cc80cff-24cc-44ef-a7fd-9f6f08e55e07",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002857,
     "end_time": "2025-09-23T09:48:11.138224",
     "exception": false,
     "start_time": "2025-09-23T09:48:11.135367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7b9d50",
   "metadata": {
    "_cell_guid": "d5003f74-e762-4cbc-90b9-237b15ca183d",
    "_uuid": "23e9d836-0604-44ee-963c-abc6156d02a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:11.145061Z",
     "iopub.status.busy": "2025-09-23T09:48:11.144820Z",
     "iopub.status.idle": "2025-09-23T09:48:44.785232Z",
     "shell.execute_reply": "2025-09-23T09:48:44.784562Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 33.645484,
     "end_time": "2025-09-23T09:48:44.786758",
     "exception": false,
     "start_time": "2025-09-23T09:48:11.141274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aris-gk3/ml_project_util.git\r\n",
      "  Cloning https://github.com/aris-gk3/ml_project_util.git to /tmp/pip-req-build-yn731u54\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aris-gk3/ml_project_util.git /tmp/pip-req-build-yn731u54\r\n",
      "  Resolved https://github.com/aris-gk3/ml_project_util.git to commit 62c6be5dba2d44b545daa677b6e6d9dcd247920b\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: ml_project_util\r\n",
      "  Building wheel for ml_project_util (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for ml_project_util: filename=ml_project_util-0.1-py3-none-any.whl size=23354 sha256=2c231b9ab75c4aa2cdfed46c11ede51aff808296356ef0c62014ecbb356be201\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tv0zf4ff/wheels/9b/33/7a/e8e8f55a4c6aa39df26369c48b9e3497c6dde4c7663912f8ef\r\n",
      "Successfully built ml_project_util\r\n",
      "Installing collected packages: ml_project_util\r\n",
      "Successfully installed ml_project_util-0.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 09:48:23.796980: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758620904.155687      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758620904.254475      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall ml_project_util -y\n",
    "# !pip install git+https://github.com/aris-gk3/ml_project_util.git\n",
    "# or for local development:\n",
    "# %pip uninstall ml_project_util -y\n",
    "%pip install git+https://github.com/aris-gk3/ml_project_util.git\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from tensorflow.keras.applications import VGG16 # type: ignore\n",
    "# Local Packages\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.train import train, freeze_layers, unfreeze_head, unfreeze_block\n",
    "from ml_project_util.quantization_util import quant_model, quant_bw_search\n",
    "from ml_project_util.quantization_util import quant_activations, quant_weights\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9cb67",
   "metadata": {
    "_cell_guid": "3993a687-9518-46d5-9f7b-266fc0848bad",
    "_uuid": "308395e4-69e1-4999-a3f0-e5d3a814d01b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003325,
     "end_time": "2025-09-23T09:48:44.794273",
     "exception": false,
     "start_time": "2025-09-23T09:48:44.790948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Variable Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311c0bf6",
   "metadata": {
    "_cell_guid": "360f9dac-b965-4199-9b5a-8e4a76736b27",
    "_uuid": "c841e233-4c85-423e-9816-1f306957b772",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:44.802052Z",
     "iopub.status.busy": "2025-09-23T09:48:44.801614Z",
     "iopub.status.idle": "2025-09-23T09:48:44.880768Z",
     "shell.execute_reply": "2025-09-23T09:48:44.880063Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.084288,
     "end_time": "2025-09-23T09:48:44.881880",
     "exception": false,
     "start_time": "2025-09-23T09:48:44.797592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_rel_path set to: catsdogsconv/CatsDogs\n"
     ]
    }
   ],
   "source": [
    "dict = path_definition(ds_rel_path='catsdogsconv/CatsDogs') # path_definition.config to read\n",
    "BASE_PATH = dict['BASE_PATH']\n",
    "PATH_DATASET = dict['PATH_DATASET']\n",
    "PATH_TEST = dict['PATH_TEST']\n",
    "PATH_RAWDATA = dict['PATH_RAWDATA']\n",
    "PATH_JOINEDDATA = dict['PATH_JOINEDDATA']\n",
    "PATH_SAVEDMODELS = dict['PATH_SAVEDMODELS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189801b",
   "metadata": {
    "_cell_guid": "cbf768a1-b43a-4eae-8f72-54ce23c4ee44",
    "_uuid": "84758484-daa9-4bb4-b8ec-fca2f3c8bef9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00358,
     "end_time": "2025-09-23T09:48:44.889002",
     "exception": false,
     "start_time": "2025-09-23T09:48:44.885422",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model for PTQ & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc54860",
   "metadata": {
    "_cell_guid": "482a2b13-c30b-48e4-980a-7ba7bb8aa97e",
    "_uuid": "c60d6c29-abb4-4d53-ad35-608ca89133c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:44.896541Z",
     "iopub.status.busy": "2025-09-23T09:48:44.896321Z",
     "iopub.status.idle": "2025-09-23T09:48:50.295803Z",
     "shell.execute_reply": "2025-09-23T09:48:50.295170Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.404777,
     "end_time": "2025-09-23T09:48:50.297229",
     "exception": false,
     "start_time": "2025-09-23T09:48:44.892452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758620927.594193      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1758620927.594902      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model_name = 'CD1_P1_014_val0.0357' \n",
    "parent_name = model_name[:3]\n",
    "model_path = '/kaggle/input/cd1_p1/keras/default/1/CD1_P1_014_val0.0357.keras'\n",
    "# model_path = f\"{PATH_SAVEDMODELS}/{parent_name}/{model_name}.keras\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44315810",
   "metadata": {
    "_cell_guid": "0a20eb5c-73ba-49c3-b54d-4a6a3f71de18",
    "_uuid": "5b883389-d4cf-49f4-8cfd-49e1bb9878a9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003591,
     "end_time": "2025-09-23T09:48:50.304913",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.301322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Try PTQ & Evaluation (Evluate Correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881ca97f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.312823Z",
     "iopub.status.busy": "2025-09-23T09:48:50.312562Z",
     "iopub.status.idle": "2025-09-23T09:48:50.315828Z",
     "shell.execute_reply": "2025-09-23T09:48:50.315214Z"
    },
    "papermill": {
     "duration": 0.008478,
     "end_time": "2025-09-23T09:48:50.316902",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.308424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quant_model(model, model_name, num_bits=8, design='sw', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f443ccd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.324553Z",
     "iopub.status.busy": "2025-09-23T09:48:50.324366Z",
     "iopub.status.idle": "2025-09-23T09:48:50.327635Z",
     "shell.execute_reply": "2025-09-23T09:48:50.326940Z"
    },
    "papermill": {
     "duration": 0.008369,
     "end_time": "2025-09-23T09:48:50.328855",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.320486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quant_model(model, model_name, num_bits=8, design='hwa', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8694ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.336496Z",
     "iopub.status.busy": "2025-09-23T09:48:50.336316Z",
     "iopub.status.idle": "2025-09-23T09:48:50.339281Z",
     "shell.execute_reply": "2025-09-23T09:48:50.338747Z"
    },
    "papermill": {
     "duration": 0.007859,
     "end_time": "2025-09-23T09:48:50.340217",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.332358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quant_model(model, model_name, num_bits=8, design='hww', batch_len=157, force=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c775ab8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.348194Z",
     "iopub.status.busy": "2025-09-23T09:48:50.348015Z",
     "iopub.status.idle": "2025-09-23T09:48:50.351043Z",
     "shell.execute_reply": "2025-09-23T09:48:50.350451Z"
    },
    "papermill": {
     "duration": 0.008238,
     "end_time": "2025-09-23T09:48:50.352183",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.343945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qw_model, acc, loss = quant_weights(model, model_name, num_bits=8, quant='symmetric', mode='eval', design='sw', batch_len=157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6190531e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.360124Z",
     "iopub.status.busy": "2025-09-23T09:48:50.359933Z",
     "iopub.status.idle": "2025-09-23T09:48:50.362858Z",
     "shell.execute_reply": "2025-09-23T09:48:50.362340Z"
    },
    "papermill": {
     "duration": 0.008118,
     "end_time": "2025-09-23T09:48:50.363909",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.355791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qw_model, acc, loss = quant_weights(model, model_name, num_bits=8, quant='symmetric', mode='eval', design='hww', batch_len=157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27804058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.372240Z",
     "iopub.status.busy": "2025-09-23T09:48:50.372063Z",
     "iopub.status.idle": "2025-09-23T09:48:50.374794Z",
     "shell.execute_reply": "2025-09-23T09:48:50.374317Z"
    },
    "papermill": {
     "duration": 0.008333,
     "end_time": "2025-09-23T09:48:50.375848",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.367515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qw_model, acc, loss = quant_weights(model, model_name, num_bits=8, quant='symmetric', mode='eval', design='hwa', batch_len=157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b2b983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.383997Z",
     "iopub.status.busy": "2025-09-23T09:48:50.383475Z",
     "iopub.status.idle": "2025-09-23T09:48:50.386284Z",
     "shell.execute_reply": "2025-09-23T09:48:50.385760Z"
    },
    "papermill": {
     "duration": 0.007942,
     "end_time": "2025-09-23T09:48:50.387326",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.379384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quant_activations(model, model_name, num_bits=8, input_shape=(224,224,3), mode='eval', range_path='0', design='hwa', batch_len=157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df3c958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.395800Z",
     "iopub.status.busy": "2025-09-23T09:48:50.395262Z",
     "iopub.status.idle": "2025-09-23T09:48:50.398303Z",
     "shell.execute_reply": "2025-09-23T09:48:50.397741Z"
    },
    "papermill": {
     "duration": 0.008384,
     "end_time": "2025-09-23T09:48:50.399384",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.391000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quant_activations(model, model_name, num_bits=8, input_shape=(224,224,3), mode='eval', range_path='0', design='hww', batch_len=157)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17dccbb2",
   "metadata": {
    "_cell_guid": "a47c79f3-b0d7-4b33-8b11-3d50d94f3ca4",
    "_uuid": "ad96a75d-795f-4cb6-a1ec-c0ae077e353c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T09:48:50.407502Z",
     "iopub.status.busy": "2025-09-23T09:48:50.407303Z",
     "iopub.status.idle": "2025-09-23T09:49:28.022982Z",
     "shell.execute_reply": "2025-09-23T09:49:28.022149Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 37.621068,
     "end_time": "2025-09-23T09:49:28.024209",
     "exception": false,
     "start_time": "2025-09-23T09:48:50.403141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758620939.174974      19 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 117\n",
      "Precise test accuracy: 0.98506\n",
      "Precise test loss: 0.04673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9850587, 0.046727076653085675)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_evaluation_precise(model, mode='val')\n",
    "model_evaluation_precise(model, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41bac13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:49:28.041658Z",
     "iopub.status.busy": "2025-09-23T09:49:28.041439Z",
     "iopub.status.idle": "2025-09-23T09:49:28.044465Z",
     "shell.execute_reply": "2025-09-23T09:49:28.043931Z"
    },
    "papermill": {
     "duration": 0.013005,
     "end_time": "2025-09-23T09:49:28.045662",
     "exception": false,
     "start_time": "2025-09-23T09:49:28.032657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = 'CD1_P1_FT3_014_val0.0314' \n",
    "# parent_name = model_name[:3]\n",
    "# model_path = '/kaggle/input/cd1_p1_search/keras/default/1/CD1_P1_FT3_014_val0.0314.keras'\n",
    "# # model_path = f\"{PATH_SAVEDMODELS}/{parent_name}/{model_name}.keras\"\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f774d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T09:49:28.063109Z",
     "iopub.status.busy": "2025-09-23T09:49:28.062638Z",
     "iopub.status.idle": "2025-09-23T09:49:28.065421Z",
     "shell.execute_reply": "2025-09-23T09:49:28.064922Z"
    },
    "papermill": {
     "duration": 0.012428,
     "end_time": "2025-09-23T09:49:28.066450",
     "exception": false,
     "start_time": "2025-09-23T09:49:28.054022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_evaluation_precise(model, mode='val')\n",
    "# model_evaluation_precise(model, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4beb70b2",
   "metadata": {
    "_cell_guid": "d85a36c6-93a5-415b-860b-3bc6646b4262",
    "_uuid": "1a896811-214d-4bdb-b5c3-4c74fd5e91c0",
    "execution": {
     "iopub.execute_input": "2025-09-23T09:49:28.084005Z",
     "iopub.status.busy": "2025-09-23T09:49:28.083788Z",
     "iopub.status.idle": "2025-09-23T10:58:11.356568Z",
     "shell.execute_reply": "2025-09-23T10:58:11.355922Z"
    },
    "papermill": {
     "duration": 4123.284137,
     "end_time": "2025-09-23T10:58:11.358979",
     "exception": false,
     "start_time": "2025-09-23T09:49:28.074842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model to 7 bits...\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "Saved activation ranges in /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 117\n",
      "Precise test accuracy: 0.50293\n",
      "Precise test loss: 0.69091\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1494.113945603282\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 5029.367317824078\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7709.599562275977\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16865.684741723042\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27995.039089282353\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33599.90548282128\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28714.505367419075\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23578.979970466997\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9488.879523612559\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7800.8183595474275\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7397.301647034429\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1576.375577541992\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 856.0136707180312\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=5, N_i=12\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 250.71682697945894\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=2, N_i=9\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4869584018562731\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.50000\n",
      "Precise test loss: 0.69426\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8716005691931885\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5236883314530993\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8086025326771544\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4162453385418927\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7001682459352561\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4934379465520311\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.50415107565628\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3682456561930214\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5332069215243644\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3139823397278876\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19473361219715518\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3821016839459767\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.5728682523880584\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6584285158559017\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22049252755884235\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.52134\n",
      "Precise test loss: 0.67854\n",
      "Quantizing model to 8 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.93917\n",
      "Precise test loss: 0.14126\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'8b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=11, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1482.3492688662482\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=11, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4989.766000360896\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7648.894053911599\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=10, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16732.88407446538\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27774.605710626587\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33335.33929791718\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28488.406899959085\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23393.31871085702\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9414.16393681246\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7739.394592936818\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=10, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7339.055177372741\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1563.9631714196141\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1698.5468111885345\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=6, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 248.7426787355262\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=3, N_i=11\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.475250067983389\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.63741\n",
      "Precise test loss: 0.57333\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'8b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8785180340280552\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5278445880519335\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8150200130952271\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4195488729747649\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7057251367760121\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4973541207310155\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.508152274669425\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37116824076598187\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5374387224888436\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3164742630590613\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19627911705586276\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.385134236993167\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2887074129098548\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6636541389976152\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2222424682537538\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.95144\n",
      "Precise test loss: 0.12507\n",
      "Quantizing model to 9 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97866\n",
      "Precise test loss: 0.05739\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1476.5361344785374\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4970.198290555559\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7618.898390955083\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16667.26492123218\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27665.685688231973\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33204.61247714103\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28376.687657214145\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23301.58020610856\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9377.245646864176\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7709.044025905693\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7310.274568834025\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1557.8299825120864\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1691.8858433015207\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=7, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 247.76721725028884\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=4, N_i=13\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4694647735991406\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.95998\n",
      "Precise test loss: 0.08968\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8819767664454884\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5299227163513506\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8182287533042635\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42120064019120096\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7085035821963901\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49931220782050767\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5101528741759976\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37262953305246216\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5395546229710831\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31772022472464817\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19705186948521655\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38665051351676216\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2898440562677676\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6662669505684718\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22311743860120953\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97599\n",
      "Precise test loss: 0.06586\n",
      "Quantizing model to 10 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97785\n",
      "Precise test loss: 0.06963\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1473.6466312799494\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4960.471875114159\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7603.988609368087\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16634.647964439162\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27611.545403127802\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33139.63280497442\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28321.155978824296\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23255.98024484416\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9358.894872604167\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7693.9578340741755\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7295.9687477599855\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1554.7813915482661\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1688.574912101322\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=8, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 247.28234989754856\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=5, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.46658910868016\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97759\n",
      "Precise test loss: 0.06882\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8837061326542051\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.530961780501059\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8198331234087816\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.422026523799419\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7098928049065791\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5002912513652538\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5111531739292839\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37336017919570225\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5406125732122029\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3183432055574416\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19743824569989346\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3874086517785597\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29041237794672403\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6675733563539002\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22355492377493738\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97785\n",
      "Precise test loss: 0.07023\n",
      "Quantizing model to 11 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97839\n",
      "Precise test loss: 0.07571\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1472.2061164888644\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4955.622928999677\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7596.555580424423\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16618.387311492497\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27584.55464515798\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33107.23824700279\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28293.47156437774\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23233.247126325252\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9349.7463927678\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7686.4368586742985\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7288.8368135002\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1553.2615661410832\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1686.9243012390527\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=9, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 247.04062717037598\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=6, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.465155492738146\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97545\n",
      "Precise test loss: 0.07425\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8845708157585634\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5314813125759134\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8206353084610407\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.422439465603528\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7105874162616737\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5007807731376268\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.511653323805927\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3737255022673223\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5411415483327627\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31865469597383833\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1976314338072319\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3877877209094585\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2906965387862022\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6682265592466144\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2237736663618013\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97946\n",
      "Precise test loss: 0.07467\n",
      "Quantizing model to 12 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97839\n",
      "Precise test loss: 0.07730\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1471.486914673286\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4953.202009151607\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7592.844512725144\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16610.26890049519\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27571.07904445199\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33091.06470609072\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28279.649643730758\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23221.897225433055\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9345.178856669721\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7682.681882192288\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7285.276072506796\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1552.5027671346634\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1686.1002053420136\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=10, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.91994293629176\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=7, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4644397352917669\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97892\n",
      "Precise test loss: 0.07663\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8850031573107425\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5317410786133404\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8210364009871702\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4226459365055825\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7109347219392208\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5010255340238133\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5119033987442486\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37390816380313235\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5414060358930427\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31881044118203666\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19772802786090113\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3879772554749079\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29083861920594134\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6685531606929715\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22388303765523326\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97839\n",
      "Precise test loss: 0.07674\n",
      "Quantizing model to 13 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97866\n",
      "Precise test loss: 0.07844\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1471.1275772093854\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4951.992436011399\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7590.990338240962\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16606.212668773456\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27564.34618021647\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33082.98386000865\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28272.74374638186\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23216.22643245981\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9342.896761710826\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7680.805769400544\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7283.497006310823\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1552.1236455798075\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.6884592601232\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=11, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.8596450259288\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=8, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.464082118750792\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97839\n",
      "Precise test loss: 0.07775\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8852193280868321\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.531870961632054\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.821236947250235\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42274917195660977\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7111083747779945\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5011479144669065\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5120284362134094\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37399949457103737\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5415382796731827\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31888831378613586\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19777632488773572\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3880720227576326\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909096594158109\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.66871646141615\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22393772330194925\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07799\n",
      "Quantizing model to 14 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97892\n",
      "Precise test loss: 0.07839\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.9479742821225\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4951.387870947791\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7590.063590549808\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16604.185295721476\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27560.980981073484\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33078.944916795364\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28269.29206236936\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23213.392074453164\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9341.756132146462\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7679.868056573124\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7282.607799009356\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.9341542300847\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.4826616213415\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=12, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.8295071129724\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=9, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4639033759698434\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97892\n",
      "Precise test loss: 0.07824\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8853274134748769\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319359031414108\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8213372203817674\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228007896821234\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7111952011973813\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012091046884533\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5120909549479897\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740451599549899\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416044015632526\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31892725008818545\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19780047340115303\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38811940639899495\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29094517952074567\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6687981117777394\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22396506612530725\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07846\n",
      "Quantizing model to 15 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07876\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.858189262634\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4951.085643768951\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.600301555695\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16603.17179481836\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27559.298689613977\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33076.92581498759\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28267.566536393508\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23211.975154958902\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9341.18592179841\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7679.399286014827\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7282.1632767729525\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.8394259047336\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.3797816444373\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=13, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.81444091587096\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=10, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4638140209447583\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97866\n",
      "Precise test loss: 0.07879\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8853814561688993\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319683738960892\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8213873569475336\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228265985448802\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112386144070747\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012396997992266\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121222143152799\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740679926469661\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416374625082877\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189467182392102\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978125476578617\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38814309821967613\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29096293957321306\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.668838936958534\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22397873753698624\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07878\n",
      "Quantizing model to 16 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07888\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.813300863047\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.934544014815\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.368678266974\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.66509076261\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27558.457620895766\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33075.91635651367\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28266.703852396306\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23211.26676007518\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.900842727338\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7679.164922194947\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.941036003985\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7920660785087\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.3283463656005\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=14, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.80690850701706\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=11, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.463769347522689\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07887\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854084775159106\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319846092734284\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214124252304167\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228395029762586\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112603210119214\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012549973546132\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.512137843998925\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740794089929543\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416539929808052\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31895645231472264\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19781858478621603\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3881549441300167\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29097181959944673\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688593495489313\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22398557324282575\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07893\n",
      "Quantizing model to 17 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07901\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7908576906832\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.858997596199\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.25287192413\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.411750332445\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27558.03710578749\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33075.41165038173\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28266.272530143273\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.912578847438\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.758309716844\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7679.047745649251\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.8299207062655\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7683872493933\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.3026299034602\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=15, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.8031424749959\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=12, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637470118341633\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07904\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854219881894161\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.531992726962098\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214249593718582\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42284595519194784\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112711743143447\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012626461323065\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121456588407476\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37408511716594833\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416622582170639\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896131935247884\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782160335039317\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.388160867085187\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29097625961256357\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.66886955584413\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2239889910957455\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07900\n",
      "Quantizing model to 18 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07902\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'18b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=21, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.779636361345\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=21, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.821225251458\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.194970078017\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=20, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.285083016635\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.826853045804\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33075.15930309171\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28266.056873952886\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.73549228688\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.68704484277\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.9891587173925\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=20, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.77436432903\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7565481058205\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.289771966694\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=16, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.80125950208446\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=13, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637358442455142\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07902\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'18b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.885428743526169\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319967858064328\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.821431226442579\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42284918129979243\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112766009655564\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012664705211531\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121495662616589\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37408797125244536\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416663908351933\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189637528713569\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782311263248176\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816382856277215\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.290978479619122\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688746589917293\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399070002220536\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07902\n",
      "Quantizing model to 19 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07904\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'19b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=22, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.774025760885\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=22, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.802339295223\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.166019486278\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=21, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.221750083532\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.72172787804\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33075.033130890646\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.949047091694\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.646950019902\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.651412813517\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.959865586701\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=21, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.746586458309\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7506286017783\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2833430718847\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=17, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.80031802640323\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=14, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637302605150913\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07904\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'19b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854321211945453\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319988152286002\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214343599779393\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285079435371475\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112793142911623\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012683827155765\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121515199721145\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740893982956939\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416684571442579\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896496963079596\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782386727352605\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816530930156473\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909795896224012\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.668877210565529\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2239915544854353\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07904\n",
      "Quantizing model to 20 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07905\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'20b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=23, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7712204767072\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=23, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.792896371138\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.151544273237\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=22, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.190083798178\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.66916559492\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.9700451511\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.895133969592\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.602679139734\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.633596900834\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.945219105164\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=22, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.7326976024215\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7476688666932\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2801286428733\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=18, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.7998472912562\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=15, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.463727468665855\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07905\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'20b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854338100287336\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5319998299396839\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214359267456196\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285160088067586\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112806709539652\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012693388127882\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121524968273423\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740901118173181\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416694902987903\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189655780105155\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978242445940482\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.388166049670961\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909801446240408\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688784863524289\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399198171705026\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07905\n",
      "Quantizing model to 21 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07905\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'21b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=24, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7698178386313\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=24, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.788174922604\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.144306687424\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=23, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.1742507008\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.642884528555\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.938502371566\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.868177485667\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.580543762982\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.624688969978\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.937895885348\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=23, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.725753194346\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7461890033846\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.278521432966\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=19, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.7996119243561\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=16, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637260727452308\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'21b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854346544458276\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320003372952258\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214367101294596\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285200414415647\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112813492853667\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.501269816861394\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121529852549562\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740904685781303\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416700068760565\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896588220037525\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782443325430926\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816641985565914\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29098042212486064\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688791242458787\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399219533285775\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 22 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'22b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=25, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7691165205965\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=25, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.785814201715\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.140687899694\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=24, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.166334163434\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.62974401417\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.922731004364\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.85469926298\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.569476090437\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.620235010922\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.934234280678\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=24, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.7222809952755\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7454490727887\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.277717829162\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=20, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.7994942410744\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=17, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637253747859171\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'22b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854350766543747\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320005909729967\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214371018213797\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228522057758968\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112816884510674\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.501270055885697\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121532294687632\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37409064695853633\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416702651646895\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189660342953051\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978245275844398\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816660494800825\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909805608752705\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688794431926037\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2239923021407615\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 23 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'23b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=26, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.76876586183\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=26, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.784633842114\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.138878507123\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=25, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.16237589758\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.623173761676\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.914845326406\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.84796015646\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.563942258126\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.618008032987\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.932403479652\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=25, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.720544896983\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7450791077554\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2773160275472\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=21, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.7994353994756\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=18, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637250258065098\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'23b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854352877586482\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320007178118822\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214372976673397\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285230659176687\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112818580339177\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012701753978483\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121533515756667\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37409073614873933\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416703943090061\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896611034277006\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782457474950507\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816669749418276\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909806302504755\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688796026659661\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399235554471336\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 24 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'24b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=27, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7685905325095\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=27, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.784043662525\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137973811162\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=26, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.160396765365\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.619888636607\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.91090248883\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.844590604407\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.56117534296\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.616894544417\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.931488079466\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=26, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.719676848145\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7448941253049\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2771151268116\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=22, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79940597868676\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=19, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637248513168686\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'24b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854353933107849\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320007812313249\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214373955903198\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.422852356999702\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.711281942825343\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702351539241\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534126291184\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740907807438409\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416704588811644\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896614836650256\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782459833203772\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816674376727006\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29098066493807795\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688796824026474\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2239923822466893\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 25 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'25b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=28, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7685028678648\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=28, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.783748572783\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137521463262\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=27, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.159407199433\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.618246074362\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.9089310704\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.84290582868\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.559791885622\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.616337800231\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.9310303794555\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=27, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.719242823805\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7448016340961\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.277014676462\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=23, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79939126829495\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=20, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637247640720636\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'25b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354460868533\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008129410462\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374445518098\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228523822036695\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112819852210556\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.501270265031962\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534431558443\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37409080304139164\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416704911672435\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661673783688\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782461012330402\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3881667669038137\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909806822818792\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.668879722270988\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399239559767728\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 26 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'26b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=29, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7684590355464\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=29, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.783601027925\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137295289332\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=28, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.15891241651\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.617424793316\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.90794536127\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.84206344089\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.559100157014\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.616059428165\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930801529471\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=28, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.719025811654\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.744755388496\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2769644512914\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=24, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79938391309972\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=21, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637247204496648\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'26b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354724748875\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008287959069\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374690325548\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228523948056533\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112820064189118\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.501270279970981\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534584192072\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.374090814190167\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705073102831\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661768843019\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978246160189372\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816677847208547\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909806909537798\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688797422051583\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399240227317124\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 27 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'27b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=30, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7684371193882\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=30, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.783527255499\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137182202372\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=29, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.15866502506\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.61701415281\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.907452506726\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.841642247015\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.558754292728\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.615920242137\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930687104483\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=29, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.718917305584\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.744732265697\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2769393387073\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=25, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79938023550224\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=22, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637246986384667\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'27b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354856689046\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008367233372\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374812729273\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228524011066451\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.71128201701784\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702874404904\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534660508887\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740908197645547\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705153818029\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661816372685\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782461896675377\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3881667842562214\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29098069528973014\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688797521722435\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399240561091824\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 28 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'28b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=31, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7684261613094\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=31, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.783490369287\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137125658893\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=30, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.15854132934\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.61680883256\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.907206079464\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.841431650082\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.558581360587\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.615850649125\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930629891991\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=30, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.71886305255\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7447207042976\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2769267824156\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=26, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79937839670356\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=23, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637246877328678\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'28b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354922659131\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008406870524\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374873931135\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228524042571411\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112820223173041\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702911752451\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534698667294\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740908225517485\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705194175627\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.31896618401375176\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782462044066207\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816678714828934\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909806974577053\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688797571557861\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399240727979175\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 29 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'29b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=32, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.76842068227\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=32, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.783471926182\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137097387154\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=31, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.158479481477\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.61670617244\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.90708286583\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.84132635162\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.558494894518\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.615815852618\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930601285745\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=31, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.718835926033\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.744714923598\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2769205042698\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=27, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79937747730423\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=24, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637246822800682\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'29b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354955644175\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.53200084266891\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374904532067\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4228524058323891\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112820249670361\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702930426225\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5121534717746498\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740908239453455\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705214354427\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661852019934\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978246211776162\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3881667885943233\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29098069854169284\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688797596475573\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.2239924081142285\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 30 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'30b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=33, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7684179427504\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=33, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.7834627046295\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.137083251285\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=32, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.158448557548\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.61665484238\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.907021259016\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.841273702386\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.558451661484\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.615798454366\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930586982622\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=32, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.7188223627745\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7447120332483\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.276917365197\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=28, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79937701760457\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=25, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637246795536687\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'30b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354972136695\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008436598388\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374919832532\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285240662001305\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112820262919021\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702939763112\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.51215347272861\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.37409082464214394\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705224443826\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661857961142\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.1978246215460933\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.38816678931734033\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.2909806990836866\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.668879760893443\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399240853144686\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Quantizing model to 31 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "'31b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=34, N_i=65\n",
      "Next input range: {'min': 0.0, 'max': 1070.0382080078125}\n",
      "HW next input range: 1470.7684165729904\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=34, N_i=65\n",
      "Next input range: {'min': 0.0, 'max': 4554.08984375}\n",
      "HW next input range: 4950.7834580938525\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 7481.80029296875}\n",
      "HW next input range: 7589.13707618335\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=33, N_i=64\n",
      "Next input range: {'min': 0.0, 'max': 12654.7890625}\n",
      "HW next input range: 16602.158433095585\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 18002.248046875}\n",
      "HW next input range: 27557.616629177348\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 18047.984375}\n",
      "HW next input range: 33074.90699045561\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 18486.67578125}\n",
      "HW next input range: 28265.841247377768\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 13831.3916015625}\n",
      "HW next input range: 23210.558430044966\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 7492.05712890625}\n",
      "HW next input range: 9340.615789755238\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 4779.42578125}\n",
      "HW next input range: 7678.930579831061\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=33, N_i=64\n",
      "Next input range: {'min': 0.0, 'max': 3781.952392578125}\n",
      "HW next input range: 7281.7188155811455\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 1468.0283203125}\n",
      "HW next input range: 1551.7447105880733\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 854.3358154296875}\n",
      "HW next input range: 1685.2769157956604\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=29, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 142.8619842529297}\n",
      "HW next input range: 246.79937678775474\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=26, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.4637246781904687\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_range.json\n",
      "'31b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8854354980382957\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5320008441553032\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.8214374927482765\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42285240701382504\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.7112820269543351\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5012702944431555\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.51215347320559\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3740908249905432\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5416705229488527\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.3189661860931746\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19044439494609833\n",
      "HW weight range: 0.19782462173033183\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2051512748003006\n",
      "HW weight range: 0.3881667896788488\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28699666261672974\n",
      "HW weight range: 0.29098069935468357\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.5777572989463806\n",
      "HW weight range: 0.6688797615163857\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.16393160820007324\n",
      "HW weight range: 0.22399240874005605\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/CD1_P1_014_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 3748 files belonging to 2 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 117\n",
      "Precise test accuracy: 0.97919\n",
      "Precise test loss: 0.07906\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.5029348731040955,\n",
      "        \"loss\": 0.690906547176927\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.9391675591468811,\n",
      "        \"loss\": 0.14126339307494198\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9786552786827087,\n",
      "        \"loss\": 0.057391141921987375\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.97785484790802,\n",
      "        \"loss\": 0.06962513714112097\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9783884882926941,\n",
      "        \"loss\": 0.07571214872852042\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9783884882926941,\n",
      "        \"loss\": 0.07730301888020569\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9786552786827087,\n",
      "        \"loss\": 0.07844229081014101\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9789220690727234,\n",
      "        \"loss\": 0.07838914501972254\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07876407478461807\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07887766062459547\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.079013304462568\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07902144610565408\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07904216445820843\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790501346005993\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790546155480873\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905850772045159\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906006671698497\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906006197221328\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906031474529052\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906050830875214\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906066145681052\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790606895473956\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906068519627832\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.079060608952276\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906065336907904\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.5,\n",
      "        \"loss\": 0.6942609549204848\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.637406587600708,\n",
      "        \"loss\": 0.5733319991044581\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9599786400794983,\n",
      "        \"loss\": 0.08967763566180444\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9775880575180054,\n",
      "        \"loss\": 0.06882248849801498\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9754535555839539,\n",
      "        \"loss\": 0.0742472256598512\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9789220690727234,\n",
      "        \"loss\": 0.07662544100663551\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9783884882926941,\n",
      "        \"loss\": 0.07775086756059568\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9789220690727234,\n",
      "        \"loss\": 0.07824102648727352\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9786552786827087,\n",
      "        \"loss\": 0.07879100297247184\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07886532408965749\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07903930877101126\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790239013898451\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07903788363062116\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905278657254633\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905918982952766\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906203859854587\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906331711153765\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906398721421076\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906446662564164\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906447671846809\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906463157364291\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906471031721192\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906465155551412\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790646617832442\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906466550108111\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.5213447213172913,\n",
      "        \"loss\": 0.6785369220894454\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.9514407515525818,\n",
      "        \"loss\": 0.12507100073718083\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9759871959686279,\n",
      "        \"loss\": 0.06586033317047701\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.97785484790802,\n",
      "        \"loss\": 0.07022813255496012\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9794557094573975,\n",
      "        \"loss\": 0.07466881431565632\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9783884882926941,\n",
      "        \"loss\": 0.0767367007356325\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07798585968982334\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07846131513041575\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07877828640877982\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07892815354089414\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07900238392084427\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07902034202016332\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07904338906200468\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905294194415487\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905629047447925\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790586456377817\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07905981824487991\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906014919041485\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906040174465803\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906053153201949\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906062178071004\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790606558758276\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906066420996297\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.07906058886573317\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9791889190673828,\n",
      "        \"loss\": 0.0790606187103096\n",
      "    }\n",
      "}\n",
      "CPU times: user 34min 9s, sys: 3min 29s, total: 37min 39s\n",
      "Wall time: 1h 8min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quant_bw_search(model, model_name, range(7,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82794d66",
   "metadata": {
    "_cell_guid": "b46cc0ab-15e2-4593-8e54-9530b0f909c3",
    "_uuid": "8dbc6ebe-a4aa-4de1-a399-5930df25ea63",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.40863,
     "end_time": "2025-09-23T10:58:12.124028",
     "exception": false,
     "start_time": "2025-09-23T10:58:11.715398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7370815,
     "sourceId": 12122965,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 379680,
     "modelInstanceId": 358362,
     "sourceId": 439529,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 380015,
     "modelInstanceId": 358702,
     "sourceId": 440980,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4211.238016,
   "end_time": "2025-09-23T10:58:15.930643",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T09:48:04.692627",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
