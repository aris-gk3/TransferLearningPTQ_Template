{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e940d3",
   "metadata": {
    "_cell_guid": "0aaedd7b-7a24-4c41-80b7-8fb76b6b2c01",
    "_uuid": "67131900-a7e4-4b80-b2f2-4d8904a6ac99",
    "collapsed": false,
    "id": "bac0c89d",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003269,
     "end_time": "2025-09-23T11:11:11.836185",
     "exception": false,
     "start_time": "2025-09-23T11:11:11.832916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387e656",
   "metadata": {
    "_cell_guid": "3247ec71-b268-4779-a56b-fd22da32ee8f",
    "_uuid": "5cc80cff-24cc-44ef-a7fd-9f6f08e55e07",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002283,
     "end_time": "2025-09-23T11:11:11.841093",
     "exception": false,
     "start_time": "2025-09-23T11:11:11.838810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6307d6dc",
   "metadata": {
    "_cell_guid": "d5003f74-e762-4cbc-90b9-237b15ca183d",
    "_uuid": "23e9d836-0604-44ee-963c-abc6156d02a5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:11.846980Z",
     "iopub.status.busy": "2025-09-23T11:11:11.846691Z",
     "iopub.status.idle": "2025-09-23T11:11:18.625353Z",
     "shell.execute_reply": "2025-09-23T11:11:18.624318Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.784259,
     "end_time": "2025-09-23T11:11:18.627694",
     "exception": false,
     "start_time": "2025-09-23T11:11:11.843435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aris-gk3/ml_project_util.git\r\n",
      "  Cloning https://github.com/aris-gk3/ml_project_util.git to /tmp/pip-req-build-ok93vzd0\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aris-gk3/ml_project_util.git /tmp/pip-req-build-ok93vzd0\r\n",
      "  Resolved https://github.com/aris-gk3/ml_project_util.git to commit 62c6be5dba2d44b545daa677b6e6d9dcd247920b\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: ml_project_util\r\n",
      "  Building wheel for ml_project_util (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for ml_project_util: filename=ml_project_util-0.1-py3-none-any.whl size=23354 sha256=b63cc2d4dbd180308df0c7c51af33858cfa3ac88504dcd2718f694a03eb90b84\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8dzewpz4/wheels/9b/33/7a/e8e8f55a4c6aa39df26369c48b9e3497c6dde4c7663912f8ef\r\n",
      "Successfully built ml_project_util\r\n",
      "Installing collected packages: ml_project_util\r\n",
      "Successfully installed ml_project_util-0.1\r\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall ml_project_util -y\n",
    "!pip install git+https://github.com/aris-gk3/ml_project_util.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc21d5ac",
   "metadata": {
    "_cell_guid": "8e53ac5c-7ba2-4e40-9c93-97a5d1c598f2",
    "_uuid": "1a0d148e-9f06-46e6-a0f4-9122fda4055d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:18.634674Z",
     "iopub.status.busy": "2025-09-23T11:11:18.634396Z",
     "iopub.status.idle": "2025-09-23T11:11:34.383760Z",
     "shell.execute_reply": "2025-09-23T11:11:34.383166Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.754424,
     "end_time": "2025-09-23T11:11:34.385157",
     "exception": false,
     "start_time": "2025-09-23T11:11:18.630733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 11:11:19.997599: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758625880.176325      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758625880.230584      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from tensorflow.keras.applications import VGG16 # type: ignore\n",
    "# Local Packages\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.train import train, freeze_layers, unfreeze_head, unfreeze_block\n",
    "from ml_project_util.quantization_util import quant_model, quant_bw_search\n",
    "from ml_project_util.model_evaluation import model_evaluation_precise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a369da",
   "metadata": {
    "_cell_guid": "3993a687-9518-46d5-9f7b-266fc0848bad",
    "_uuid": "308395e4-69e1-4999-a3f0-e5d3a814d01b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002584,
     "end_time": "2025-09-23T11:11:34.390910",
     "exception": false,
     "start_time": "2025-09-23T11:11:34.388326",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Variable Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76db25a3",
   "metadata": {
    "_cell_guid": "360f9dac-b965-4199-9b5a-8e4a76736b27",
    "_uuid": "c841e233-4c85-423e-9816-1f306957b772",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:34.397404Z",
     "iopub.status.busy": "2025-09-23T11:11:34.396971Z",
     "iopub.status.idle": "2025-09-23T11:11:34.440274Z",
     "shell.execute_reply": "2025-09-23T11:11:34.439476Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.047914,
     "end_time": "2025-09-23T11:11:34.441520",
     "exception": false,
     "start_time": "2025-09-23T11:11:34.393606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_rel_path set to: oxford-flowers-17-restructured\n"
     ]
    }
   ],
   "source": [
    "dict = path_definition(ds_rel_path='oxford-flowers-17-restructured')\n",
    "BASE_PATH = dict['BASE_PATH']\n",
    "PATH_DATASET = dict['PATH_DATASET']\n",
    "PATH_TEST = dict['PATH_TEST']\n",
    "PATH_RAWDATA = dict['PATH_RAWDATA']\n",
    "PATH_JOINEDDATA = dict['PATH_JOINEDDATA']\n",
    "PATH_SAVEDMODELS = dict['PATH_SAVEDMODELS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985d33c",
   "metadata": {
    "_cell_guid": "cbf768a1-b43a-4eae-8f72-54ce23c4ee44",
    "_uuid": "84758484-daa9-4bb4-b8ec-fca2f3c8bef9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.002598,
     "end_time": "2025-09-23T11:11:34.447105",
     "exception": false,
     "start_time": "2025-09-23T11:11:34.444507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Model for PTQ & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbe831d",
   "metadata": {
    "_cell_guid": "482a2b13-c30b-48e4-980a-7ba7bb8aa97e",
    "_uuid": "c60d6c29-abb4-4d53-ad35-608ca89133c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:34.453358Z",
     "iopub.status.busy": "2025-09-23T11:11:34.453134Z",
     "iopub.status.idle": "2025-09-23T11:11:38.380706Z",
     "shell.execute_reply": "2025-09-23T11:11:38.380090Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.932236,
     "end_time": "2025-09-23T11:11:38.382061",
     "exception": false,
     "start_time": "2025-09-23T11:11:34.449825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758625895.466653      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "model_name = 'OF2_P1_008_val0.3397_025_val0.1014'\n",
    "model_path = \"/kaggle/input/of2_p1_008_val0.3397_025_val0.1014/keras/default/1/OF2_P1_008_val0.3397_025_val0.1014.keras\"\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c05890c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:38.389093Z",
     "iopub.status.busy": "2025-09-23T11:11:38.388843Z",
     "iopub.status.idle": "2025-09-23T11:11:38.392575Z",
     "shell.execute_reply": "2025-09-23T11:11:38.391860Z"
    },
    "papermill": {
     "duration": 0.008383,
     "end_time": "2025-09-23T11:11:38.393650",
     "exception": false,
     "start_time": "2025-09-23T11:11:38.385267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ml_project_util.quantization_util import complete_dict_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234194e6",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:38.400055Z",
     "iopub.status.busy": "2025-09-23T11:11:38.399838Z",
     "iopub.status.idle": "2025-09-23T11:12:00.798063Z",
     "shell.execute_reply": "2025-09-23T11:12:00.797237Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 22.403187,
     "end_time": "2025-09-23T11:12:00.799672",
     "exception": false,
     "start_time": "2025-09-23T11:11:38.396485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the Input Range...\n",
      "Input tensor range over 300 images:\n",
      "min = -123.68000030517578, max = 151.06100463867188\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758625902.558345      19 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer: min = -151.0610, max = 151.0610\n",
      "block1_conv1: min = 0.0000, max = 1029.2603\n",
      "block1_conv2: min = 0.0000, max = 4360.5776\n",
      "block2_conv1: min = 0.0000, max = 8838.3828\n",
      "block2_conv2: min = 0.0000, max = 12915.8730\n",
      "block3_conv1: min = 0.0000, max = 15248.2734\n",
      "block3_conv2: min = 0.0000, max = 17254.2285\n",
      "block3_conv3: min = 0.0000, max = 18250.2715\n",
      "block4_conv1: min = 0.0000, max = 13415.9688\n",
      "block4_conv2: min = 0.0000, max = 7997.4980\n",
      "block4_conv3: min = 0.0000, max = 4699.6680\n",
      "block5_conv1: min = 0.0000, max = 3049.4363\n",
      "block5_conv2: min = 0.0000, max = 1789.3046\n",
      "block5_conv3: min = 0.0000, max = 752.4697\n",
      "dense: min = 0.0000, max = 57.0123\n",
      "dense_1: min = 0.0000, max = 1.0000\n",
      "Saved activation ranges in /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "{\n",
      "  \"block1_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.6714000701904297,\n",
      "      \"max\": 0.6085159182548523\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.015828926116228104,\n",
      "      \"max\": 2.0640370845794678\n",
      "    }\n",
      "  },\n",
      "  \"block1_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.21561293303966522,\n",
      "      \"max\": 0.2891709506511688\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -1.027151346206665,\n",
      "      \"max\": 0.9052184224128723\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.33594822883605957,\n",
      "      \"max\": 0.41661107540130615\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.17922063171863556,\n",
      "      \"max\": 0.36547425389289856\n",
      "    }\n",
      "  },\n",
      "  \"block2_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.1819043755531311,\n",
      "      \"max\": 0.277375727891922\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5953347682952881,\n",
      "      \"max\": 0.6337577700614929\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.183063343167305,\n",
      "      \"max\": 0.5444108247756958\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.20097896456718445,\n",
      "      \"max\": 0.34949612617492676\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17710502445697784,\n",
      "      \"max\": 0.45931634306907654\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1812487542629242,\n",
      "      \"max\": 0.2748450040817261\n",
      "    }\n",
      "  },\n",
      "  \"block3_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.17968426644802094,\n",
      "      \"max\": 0.3915373682975769\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.1428879201412201,\n",
      "      \"max\": 0.5947717428207397\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12409957498311996,\n",
      "      \"max\": 0.3138822615146637\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.14548234641551971,\n",
      "      \"max\": 0.31484508514404297\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.10524698346853256,\n",
      "      \"max\": 0.337660014629364\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.08428452908992767,\n",
      "      \"max\": 0.18237581849098206\n",
      "    }\n",
      "  },\n",
      "  \"block4_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.12739665806293488,\n",
      "      \"max\": 0.2562357187271118\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.19835947453975677,\n",
      "      \"max\": 0.33766546845436096\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.11339876800775528,\n",
      "      \"max\": 0.19090983271598816\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.3513747453689575,\n",
      "      \"max\": 0.6395493745803833\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv2\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.13698555529117584,\n",
      "      \"max\": 0.20506678521633148\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.9174985289573669,\n",
      "      \"max\": 0.7598058581352234\n",
      "    }\n",
      "  },\n",
      "  \"block5_conv3\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.09292663633823395,\n",
      "      \"max\": 0.28678205609321594\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.5002684593200684,\n",
      "      \"max\": 9.431780815124512\n",
      "    }\n",
      "  },\n",
      "  \"dense\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.14636802673339844,\n",
      "      \"max\": 0.1606535166501999\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.026490479707717896,\n",
      "      \"max\": 0.022414207458496094\n",
      "    }\n",
      "  },\n",
      "  \"dense_1\": {\n",
      "    \"weight\": {\n",
      "      \"min\": -0.21307024359703064,\n",
      "      \"max\": 0.1843913495540619\n",
      "    },\n",
      "    \"bias\": {\n",
      "      \"min\": -0.041059911251068115,\n",
      "      \"max\": 0.05930854752659798\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=11, N_i=19\n",
      "tmp: 0.7238033539385977\n",
      "Input: {'min': -151.06100463867188, 'max': 151.06100463867188}\n",
      "Weight range: 0.6085159182548523\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1482.3492688662482\n",
      "Accumulator bitwidth 19\n",
      "Precision bitwidth 8\n",
      "Shift right by 11\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=11, N_i=19\n",
      "tmp: 2.3435603629097144\n",
      "Input: {'min': 0.0, 'max': 1029.26025390625}\n",
      "Weight range: 0.2891709506511688\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4799.611623239095\n",
      "Accumulator bitwidth 19\n",
      "Precision bitwidth 8\n",
      "Shift right by 11\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=10, N_i=18\n",
      "tmp: 14.304448335466807\n",
      "Input: {'min': 0.0, 'max': 4360.57763671875}\n",
      "Weight range: 0.41661107540130615\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14647.75509551801\n",
      "Accumulator bitwidth 18\n",
      "Precision bitwidth 8\n",
      "Shift right by 10\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=10, N_i=18\n",
      "tmp: 19.303565874052286\n",
      "Input: {'min': 0.0, 'max': 8838.3828125}\n",
      "Weight range: 0.277375727891922\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19766.85145502954\n",
      "Accumulator bitwidth 18\n",
      "Precision bitwidth 8\n",
      "Shift right by 10\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=9, N_i=17\n",
      "tmp: 55.36646533974329\n",
      "Input: {'min': 0.0, 'max': 12915.873046875}\n",
      "Weight range: 0.5444108247756958\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28347.630253948566\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=9, N_i=17\n",
      "tmp: 55.14788341283336\n",
      "Input: {'min': 0.0, 'max': 15248.2734375}\n",
      "Weight range: 0.45931634306907654\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28235.71630737068\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=9, N_i=17\n",
      "tmp: 53.194293110337156\n",
      "Input: {'min': 0.0, 'max': 17254.228515625}\n",
      "Weight range: 0.3915373682975769\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27235.478072492624\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=9, N_i=17\n",
      "tmp: 45.10579910844255\n",
      "Input: {'min': 0.0, 'max': 18250.271484375}\n",
      "Weight range: 0.3138822615146637\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 23094.169143522584\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=8, N_i=16\n",
      "tmp: 35.669576412536145\n",
      "Input: {'min': 0.0, 'max': 13415.96875}\n",
      "Weight range: 0.337660014629364\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9131.411561609253\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=9, N_i=17\n",
      "tmp: 16.13578472487944\n",
      "Input: {'min': 0.0, 'max': 7997.498046875}\n",
      "Weight range: 0.2562357187271118\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8261.521779138273\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=9, N_i=17\n",
      "tmp: 7.064667919171263\n",
      "Input: {'min': 0.0, 'max': 4699.66796875}\n",
      "Weight range: 0.19090983271598816\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3617.1099746156865\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=9, N_i=17\n",
      "tmp: 4.923922004074498\n",
      "Input: {'min': 0.0, 'max': 3049.436279296875}\n",
      "Weight range: 0.20506678521633148\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2521.048066086143\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=8, N_i=16\n",
      "tmp: 4.040475923235465\n",
      "Input: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "Weight range: 0.28678205609321594\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1034.361836348279\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=6, N_i=14\n",
      "tmp: 0.9518653381068304\n",
      "Input: {'min': 0.0, 'max': 752.4696655273438}\n",
      "Weight range: 0.1606535166501999\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.919381638837145\n",
      "Accumulator bitwidth 14\n",
      "Precision bitwidth 8\n",
      "Shift right by 6\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=4, N_i=12\n",
      "tmp: 0.08277611763552019\n",
      "Input: {'min': 0.0, 'max': 57.01225662231445}\n",
      "Weight range: 0.1843913495540619\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.324417882168323\n",
      "Accumulator bitwidth 12\n",
      "Precision bitwidth 8\n",
      "Shift right by 4\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "tmp: 865.3196273833745\n",
      "Input: {'min': -151.06100463867188, 'max': 151.06100463867188}\n",
      "Next input: {'min': 0.0, 'max': 1029.26025390625}\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8450386986165767\n",
      "Accumulator bitwidth 18\n",
      "Precision bitwidth 8\n",
      "Shift right by 10\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "tmp: 538.0498836533558\n",
      "Input: {'min': 0.0, 'max': 1029.26025390625}\n",
      "Next input: {'min': 0.0, 'max': 4360.57763671875}\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5254393395052303\n",
      "Accumulator bitwidth 18\n",
      "Precision bitwidth 8\n",
      "Shift right by 10\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "tmp: 257.41420304859895\n",
      "Input: {'min': 0.0, 'max': 4360.57763671875}\n",
      "Next input: {'min': 0.0, 'max': 8838.3828125}\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5027621153292948\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "tmp: 185.5900464769697\n",
      "Input: {'min': 0.0, 'max': 8838.3828125}\n",
      "Next input: {'min': 0.0, 'max': 12915.873046875}\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3624805595253314\n",
      "Accumulator bitwidth 17\n",
      "Precision bitwidth 8\n",
      "Shift right by 9\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "tmp: 149.93417166105115\n",
      "Input: {'min': 0.0, 'max': 12915.873046875}\n",
      "Next input: {'min': 0.0, 'max': 15248.2734375}\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5856803580509811\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "tmp: 143.70722235970362\n",
      "Input: {'min': 0.0, 'max': 15248.2734375}\n",
      "Next input: {'min': 0.0, 'max': 17254.228515625}\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5613563373425923\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "tmp: 134.3313887616996\n",
      "Input: {'min': 0.0, 'max': 17254.228515625}\n",
      "Next input: {'min': 0.0, 'max': 18250.271484375}\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5247319873503891\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "tmp: 93.35905127267478\n",
      "Input: {'min': 0.0, 'max': 18250.271484375}\n",
      "Next input: {'min': 0.0, 'max': 13415.96875}\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36468379403388584\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "tmp: 75.70696316306827\n",
      "Input: {'min': 0.0, 'max': 13415.96875}\n",
      "Next input: {'min': 0.0, 'max': 7997.498046875}\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5914606497114708\n",
      "Accumulator bitwidth 15\n",
      "Precision bitwidth 8\n",
      "Shift right by 7\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "tmp: 74.6305692771592\n",
      "Input: {'min': 0.0, 'max': 7997.498046875}\n",
      "Next input: {'min': 0.0, 'max': 4699.66796875}\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2915256612389031\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "tmp: 82.4054826949211\n",
      "Input: {'min': 0.0, 'max': 4699.66796875}\n",
      "Next input: {'min': 0.0, 'max': 3049.436279296875}\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32189641677703557\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "tmp: 74.51924191771164\n",
      "Input: {'min': 0.0, 'max': 3049.436279296875}\n",
      "Next input: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2910907887410611\n",
      "Accumulator bitwidth 16\n",
      "Precision bitwidth 8\n",
      "Shift right by 8\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "tmp: 53.408262275921594\n",
      "Input: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "Next input: {'min': 0.0, 'max': 752.4696655273438}\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.41725204903063745\n",
      "Accumulator bitwidth 15\n",
      "Precision bitwidth 8\n",
      "Shift right by 7\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "tmp: 9.622390008186747\n",
      "Input: {'min': 0.0, 'max': 752.4696655273438}\n",
      "Next input: {'min': 0.0, 'max': 57.01225662231445}\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30069968775583583\n",
      "Accumulator bitwidth 13\n",
      "Precision bitwidth 8\n",
      "Shift right by 5\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "tmp: 2.2275911799339743\n",
      "Input: {'min': 0.0, 'max': 57.01225662231445}\n",
      "Next input: {'min': 0.0, 'max': 1.0}\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2784488974917468\n",
      "Accumulator bitwidth 11\n",
      "Precision bitwidth 8\n",
      "Shift right by 3\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "{\n",
      "  \"activation_range\": {\n",
      "    \"input_layer\": {\n",
      "      \"min\": -151.06100463867188,\n",
      "      \"max\": 151.06100463867188\n",
      "    },\n",
      "    \"block1_conv1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 1029.26025390625\n",
      "    },\n",
      "    \"block1_conv2\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 4360.57763671875\n",
      "    },\n",
      "    \"block2_conv1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 8838.3828125\n",
      "    },\n",
      "    \"block2_conv2\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 12915.873046875\n",
      "    },\n",
      "    \"block3_conv1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 15248.2734375\n",
      "    },\n",
      "    \"block3_conv2\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 17254.228515625\n",
      "    },\n",
      "    \"block3_conv3\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 18250.271484375\n",
      "    },\n",
      "    \"block4_conv1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 13415.96875\n",
      "    },\n",
      "    \"block4_conv2\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 7997.498046875\n",
      "    },\n",
      "    \"block4_conv3\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 4699.66796875\n",
      "    },\n",
      "    \"block5_conv1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 3049.436279296875\n",
      "    },\n",
      "    \"block5_conv2\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 1789.3045654296875\n",
      "    },\n",
      "    \"block5_conv3\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 752.4696655273438\n",
      "    },\n",
      "    \"dense\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 57.01225662231445\n",
      "    },\n",
      "    \"dense_1\": {\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"wt_range\": {\n",
      "    \"block1_conv1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.6714000701904297,\n",
      "        \"max\": 0.6085159182548523\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.015828926116228104,\n",
      "        \"max\": 2.0640370845794678\n",
      "      }\n",
      "    },\n",
      "    \"block1_conv2\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.21561293303966522,\n",
      "        \"max\": 0.2891709506511688\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -1.027151346206665,\n",
      "        \"max\": 0.9052184224128723\n",
      "      }\n",
      "    },\n",
      "    \"block2_conv1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.33594822883605957,\n",
      "        \"max\": 0.41661107540130615\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.17922063171863556,\n",
      "        \"max\": 0.36547425389289856\n",
      "      }\n",
      "    },\n",
      "    \"block2_conv2\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.1819043755531311,\n",
      "        \"max\": 0.277375727891922\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.5953347682952881,\n",
      "        \"max\": 0.6337577700614929\n",
      "      }\n",
      "    },\n",
      "    \"block3_conv1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.183063343167305,\n",
      "        \"max\": 0.5444108247756958\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.20097896456718445,\n",
      "        \"max\": 0.34949612617492676\n",
      "      }\n",
      "    },\n",
      "    \"block3_conv2\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.17710502445697784,\n",
      "        \"max\": 0.45931634306907654\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.1812487542629242,\n",
      "        \"max\": 0.2748450040817261\n",
      "      }\n",
      "    },\n",
      "    \"block3_conv3\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.17968426644802094,\n",
      "        \"max\": 0.3915373682975769\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.1428879201412201,\n",
      "        \"max\": 0.5947717428207397\n",
      "      }\n",
      "    },\n",
      "    \"block4_conv1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.12409957498311996,\n",
      "        \"max\": 0.3138822615146637\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.14548234641551971,\n",
      "        \"max\": 0.31484508514404297\n",
      "      }\n",
      "    },\n",
      "    \"block4_conv2\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.10524698346853256,\n",
      "        \"max\": 0.337660014629364\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.08428452908992767,\n",
      "        \"max\": 0.18237581849098206\n",
      "      }\n",
      "    },\n",
      "    \"block4_conv3\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.12739665806293488,\n",
      "        \"max\": 0.2562357187271118\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.19835947453975677,\n",
      "        \"max\": 0.33766546845436096\n",
      "      }\n",
      "    },\n",
      "    \"block5_conv1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.11339876800775528,\n",
      "        \"max\": 0.19090983271598816\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.3513747453689575,\n",
      "        \"max\": 0.6395493745803833\n",
      "      }\n",
      "    },\n",
      "    \"block5_conv2\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.13698555529117584,\n",
      "        \"max\": 0.20506678521633148\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.9174985289573669,\n",
      "        \"max\": 0.7598058581352234\n",
      "      }\n",
      "    },\n",
      "    \"block5_conv3\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.09292663633823395,\n",
      "        \"max\": 0.28678205609321594\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.5002684593200684,\n",
      "        \"max\": 9.431780815124512\n",
      "      }\n",
      "    },\n",
      "    \"dense\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.14636802673339844,\n",
      "        \"max\": 0.1606535166501999\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.026490479707717896,\n",
      "        \"max\": 0.022414207458496094\n",
      "      }\n",
      "    },\n",
      "    \"dense_1\": {\n",
      "      \"weight\": {\n",
      "        \"min\": -0.21307024359703064,\n",
      "        \"max\": 0.1843913495540619\n",
      "      },\n",
      "      \"bias\": {\n",
      "        \"min\": -0.041059911251068115,\n",
      "        \"max\": 0.05930854752659798\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"activation_hw_range\": {\n",
      "    \"8b\": {\n",
      "      \"input_layer\": {\n",
      "        \"min\": -151.06100463867188,\n",
      "        \"max\": 151.06100463867188\n",
      "      },\n",
      "      \"block1_conv1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 1482.3492688662482\n",
      "      },\n",
      "      \"block1_conv2\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 4799.611623239095\n",
      "      },\n",
      "      \"block2_conv1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 14647.75509551801\n",
      "      },\n",
      "      \"block2_conv2\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 19766.85145502954\n",
      "      },\n",
      "      \"block3_conv1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 28347.630253948566\n",
      "      },\n",
      "      \"block3_conv2\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 28235.71630737068\n",
      "      },\n",
      "      \"block3_conv3\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 27235.478072492624\n",
      "      },\n",
      "      \"block4_conv1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 23094.169143522584\n",
      "      },\n",
      "      \"block4_conv2\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 9131.411561609253\n",
      "      },\n",
      "      \"block4_conv3\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 8261.521779138273\n",
      "      },\n",
      "      \"block5_conv1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 3617.1099746156865\n",
      "      },\n",
      "      \"block5_conv2\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 2521.048066086143\n",
      "      },\n",
      "      \"block5_conv3\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 1034.361836348279\n",
      "      },\n",
      "      \"dense\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 60.919381638837145\n",
      "      },\n",
      "      \"dense_1\": {\n",
      "        \"min\": 0,\n",
      "        \"max\": 1.324417882168323\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"wt_hw_range\": {\n",
      "    \"8b\": {\n",
      "      \"block1_conv1\": {\n",
      "        \"min\": -0.8450386986165767,\n",
      "        \"max\": 0.8450386986165767\n",
      "      },\n",
      "      \"block1_conv2\": {\n",
      "        \"min\": -0.5254393395052303,\n",
      "        \"max\": 0.5254393395052303\n",
      "      },\n",
      "      \"block2_conv1\": {\n",
      "        \"min\": -0.5027621153292948,\n",
      "        \"max\": 0.5027621153292948\n",
      "      },\n",
      "      \"block2_conv2\": {\n",
      "        \"min\": -0.3624805595253314,\n",
      "        \"max\": 0.3624805595253314\n",
      "      },\n",
      "      \"block3_conv1\": {\n",
      "        \"min\": -0.5856803580509811,\n",
      "        \"max\": 0.5856803580509811\n",
      "      },\n",
      "      \"block3_conv2\": {\n",
      "        \"min\": -0.5613563373425923,\n",
      "        \"max\": 0.5613563373425923\n",
      "      },\n",
      "      \"block3_conv3\": {\n",
      "        \"min\": -0.5247319873503891,\n",
      "        \"max\": 0.5247319873503891\n",
      "      },\n",
      "      \"block4_conv1\": {\n",
      "        \"min\": -0.36468379403388584,\n",
      "        \"max\": 0.36468379403388584\n",
      "      },\n",
      "      \"block4_conv2\": {\n",
      "        \"min\": -0.5914606497114708,\n",
      "        \"max\": 0.5914606497114708\n",
      "      },\n",
      "      \"block4_conv3\": {\n",
      "        \"min\": -0.2915256612389031,\n",
      "        \"max\": 0.2915256612389031\n",
      "      },\n",
      "      \"block5_conv1\": {\n",
      "        \"min\": -0.32189641677703557,\n",
      "        \"max\": 0.32189641677703557\n",
      "      },\n",
      "      \"block5_conv2\": {\n",
      "        \"min\": -0.2910907887410611,\n",
      "        \"max\": 0.2910907887410611\n",
      "      },\n",
      "      \"block5_conv3\": {\n",
      "        \"min\": -0.41725204903063745,\n",
      "        \"max\": 0.41725204903063745\n",
      "      },\n",
      "      \"dense\": {\n",
      "        \"min\": -0.30069968775583583,\n",
      "        \"max\": 0.30069968775583583\n",
      "      },\n",
      "      \"dense_1\": {\n",
      "        \"min\": -0.2784488974917468,\n",
      "        \"max\": 0.2784488974917468\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_complete_dict.json\n"
     ]
    }
   ],
   "source": [
    "complete_dict_search(model, model_name, mode='sv', force=1, debug=1, num_bits=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7fce0",
   "metadata": {
    "_cell_guid": "0a20eb5c-73ba-49c3-b54d-4a6a3f71de18",
    "_uuid": "5b883389-d4cf-49f4-8cfd-49e1bb9878a9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.003369,
     "end_time": "2025-09-23T11:12:00.808993",
     "exception": false,
     "start_time": "2025-09-23T11:12:00.805624",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Try PTQ & Evaluation (Evluate Correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32ebaf0b",
   "metadata": {
    "_cell_guid": "a47c79f3-b0d7-4b33-8b11-3d50d94f3ca4",
    "_uuid": "ad96a75d-795f-4cb6-a1ec-c0ae077e353c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:00.817348Z",
     "iopub.status.busy": "2025-09-23T11:12:00.816666Z",
     "iopub.status.idle": "2025-09-23T11:12:06.931044Z",
     "shell.execute_reply": "2025-09-23T11:12:06.930097Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.119964,
     "end_time": "2025-09-23T11:12:06.932421",
     "exception": false,
     "start_time": "2025-09-23T11:12:00.812457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 10\n",
      "Precise test accuracy: 0.95294\n",
      "Precise test loss: 0.28112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9529412, 0.2811193805407075)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_evaluation_precise(model, mode='val')\n",
    "model_evaluation_precise(model, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd5ee4e",
   "metadata": {
    "_cell_guid": "d85a36c6-93a5-415b-860b-3bc6646b4262",
    "_uuid": "1a896811-214d-4bdb-b5c3-4c74fd5e91c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:06.943176Z",
     "iopub.status.busy": "2025-09-23T11:12:06.942510Z",
     "iopub.status.idle": "2025-09-23T11:14:49.941322Z",
     "shell.execute_reply": "2025-09-23T11:14:49.940529Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 163.006797,
     "end_time": "2025-09-23T11:14:49.944314",
     "exception": false,
     "start_time": "2025-09-23T11:12:06.937517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model to 7 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.19412\n",
      "Precise test loss: 2.48868\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'7b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1494.113945603282\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4837.7037789790875\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14764.007120085615\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19923.731228482156\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28572.61144644022\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28459.809293937116\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27451.632660369552\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 23277.456200217206\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9203.883081939486\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8327.089412306037\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3645.8171966364457\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2541.0563840709538\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1042.571057271678\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=5, N_i=12\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 61.402868794700936\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=3, N_i=10\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.334929135201405\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.06176\n",
      "Precise test loss: 2.96151\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'7b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8383848505959737\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5213020218713308\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4988033585156783\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.35962638189127366\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5810687016883749\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5569362087020995\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5206002394184963\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36181226809661116\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5868034792413017\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.28923018359135266\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.31936179932209824\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2887987352864071\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4139665998256718\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.2983319736790182\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.27625638648787476\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.29706\n",
      "Precise test loss: 2.26863\n",
      "Quantizing model to 8 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.88235\n",
      "Precise test loss: 0.45197\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read activation_hw_range json dictionary from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json and it has values for 8 bits.\n",
      "{\n",
      "    \"8b\": {\n",
      "        \"input_layer\": {\n",
      "            \"min\": -151.06100463867188,\n",
      "            \"max\": 151.06100463867188\n",
      "        },\n",
      "        \"block1_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1482.3492688662482\n",
      "        },\n",
      "        \"block1_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 4799.611623239095\n",
      "        },\n",
      "        \"block2_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 14647.75509551801\n",
      "        },\n",
      "        \"block2_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 19766.85145502954\n",
      "        },\n",
      "        \"block3_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 28347.630253948566\n",
      "        },\n",
      "        \"block3_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 28235.71630737068\n",
      "        },\n",
      "        \"block3_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 27235.478072492624\n",
      "        },\n",
      "        \"block4_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 23094.169143522584\n",
      "        },\n",
      "        \"block4_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 9131.411561609253\n",
      "        },\n",
      "        \"block4_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 8261.521779138273\n",
      "        },\n",
      "        \"block5_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 3617.1099746156865\n",
      "        },\n",
      "        \"block5_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 2521.048066086143\n",
      "        },\n",
      "        \"block5_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1034.361836348279\n",
      "        },\n",
      "        \"dense\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 60.919381638837145\n",
      "        },\n",
      "        \"dense_1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1.324417882168323\n",
      "        }\n",
      "    },\n",
      "    \"7b\": {\n",
      "        \"input_layer\": {\n",
      "            \"min\": -151.06100463867188,\n",
      "            \"max\": 151.06100463867188\n",
      "        },\n",
      "        \"block1_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1494.113945603282\n",
      "        },\n",
      "        \"block1_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 4837.7037789790875\n",
      "        },\n",
      "        \"block2_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 14764.007120085615\n",
      "        },\n",
      "        \"block2_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 19923.731228482156\n",
      "        },\n",
      "        \"block3_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 28572.61144644022\n",
      "        },\n",
      "        \"block3_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 28459.809293937116\n",
      "        },\n",
      "        \"block3_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 27451.632660369552\n",
      "        },\n",
      "        \"block4_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 23277.456200217206\n",
      "        },\n",
      "        \"block4_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 9203.883081939486\n",
      "        },\n",
      "        \"block4_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 8327.089412306037\n",
      "        },\n",
      "        \"block5_conv1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 3645.8171966364457\n",
      "        },\n",
      "        \"block5_conv2\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 2541.0563840709538\n",
      "        },\n",
      "        \"block5_conv3\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1042.571057271678\n",
      "        },\n",
      "        \"dense\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 61.402868794700936\n",
      "        },\n",
      "        \"dense_1\": {\n",
      "            \"min\": 0,\n",
      "            \"max\": 1.334929135201405\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.70294\n",
      "Precise test loss: 1.10303\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read wt_hw_range json dictionary from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json and it has values for 8 bits.\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.88529\n",
      "Precise test loss: 0.45205\n",
      "Quantizing model to 9 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.91471\n",
      "Precise test loss: 0.32321\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1476.5361344785374\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4780.789616873451\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14590.312918672842\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19689.33439050001\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28236.4630764821\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28124.98800812609\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27128.67227612991\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 23003.6037743323\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9095.602104504904\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8229.1236545142\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3602.9252296171935\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2511.1616030818836\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1030.305515421423\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=7, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.68048210299857\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=5, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3192240865519766\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.91176\n",
      "Precise test loss: 0.34908\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8483656226268781\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.52750799832218\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.504741493736103\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3639076483423603\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5879861862322842\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5635664016628388\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5267978613163355\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3661195570025232\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5937892349465553\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2926734000626783\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.3231637255045042\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2922368154683881\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4188947736331203\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.3018835447942446\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2795451529936828\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.92059\n",
      "Precise test loss: 0.33100\n",
      "Quantizing model to 10 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.92941\n",
      "Precise test loss: 0.35674\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1473.6466312799494\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4771.433864198552\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14561.76044720773\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19650.803403434456\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28181.205810187614\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28069.948892650307\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27075.582897898734\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22958.586937200533\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9077.80249177593\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8213.019694329241\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3595.874495312659\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2506.247392508338\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1028.2892619665865\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=8, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.56173360573243\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=6, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3166424347191938\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.92647\n",
      "Precise test loss: 0.35243\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8500290846320289\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5285423277306549\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5057311829395071\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3646211927508747\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5891391003229357\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5646714338229619\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5278307982993087\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36683743848684186\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5949535275640976\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2932472694745659\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32379737986823853\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.29280982883205164\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4197161359343617\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30247547331344904\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.28009328074465084\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93235\n",
      "Precise test loss: 0.34049\n",
      "Quantizing model to 11 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.34922\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1472.2061164888644\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4766.769705973529\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14547.526077269109\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19631.59440695016\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28153.65819942497\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28042.510037427775\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27049.116052446243\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22936.144525727217\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9068.928784550344\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8204.991327081607\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3592.3594664804864\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2503.7974928089166\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1027.2840916225332\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=9, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.50253347513054\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=7, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3153553942160472\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.34255\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8508608156346043\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5290594924348923\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5062260275412093\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3649779649551319\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5897155573682614\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5652239499030236\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5283472667907954\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36719637922900117\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5955356738728688\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2935342041805097\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.3241142070501057\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.29309633551388337\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4201268170849824\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30277143757305125\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2803673446201348\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.94118\n",
      "Precise test loss: 0.35277\n",
      "Quantizing model to 12 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.94118\n",
      "Precise test loss: 0.35333\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1471.486914673286\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4764.441044661378\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14540.419322956814\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19622.00398467026\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28139.904580372975\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28028.81071645199\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27035.902024086474\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22924.939765333605\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9064.49843340987\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8200.983026482154\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3590.604527806094\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2502.5743381959173\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.7822430189071\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=10, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.47297679048221\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=8, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3147128170815987\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.94118\n",
      "Precise test loss: 0.35443\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8512766811358919\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.529318074787011\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5064734498420602\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3651563510572605\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5900037858909243\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5655002079430543\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5286055010365386\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36737584960008085\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5958267470272544\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2936776715334816\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32427262064103923\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2932395888547992\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.42033215766029275\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30291941970285235\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2805043765578768\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.35651\n",
      "Quantizing model to 13 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36034\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1471.1275772093854\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4763.277566994794\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14536.868549007373\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19617.212286505506\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28133.032808802676\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28021.96607403039\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27029.29985021002\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22919.341489444636\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9062.284880678877\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8198.980344424404\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3589.7277013035778\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2501.9632089314005\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.5315025444215\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=11, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.458209274782455\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=9, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3143917638906142\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.35852\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8514846138865357\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5294473659630704\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5065971609924858\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3652455441083248\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5901479001522557\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5656383369630698\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5287346181594103\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3674655847856207\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5959722836044471\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29374940520996756\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32435182743650603\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2933112155252572\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4204348279479479\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.3029934107677529\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.28057289252674783\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36084\n",
      "Quantizing model to 14 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36255\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1470.9479742821225\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4762.696041226634\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14535.093812278157\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19614.817314916385\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28129.598181430098\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28018.54500626406\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27025.999972313522\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22916.54337670023\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9061.178509676476\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8197.979370142337\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3589.289448623648\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2501.657756213914\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.4061782247359\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=12, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.450828221275586\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=10, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3142312960888938\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36132\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8515885802618577\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5295120115511001\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5066590165676985\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.365290140633857\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5902199572829215\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5657074014730775\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5287991767208461\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36751045237839064\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5960450518930435\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29378527204821053\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32439143083423944\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.29334702886048614\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4204861630917755\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30303040630020317\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.28060715051118335\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36220\n",
      "Quantizing model to 15 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36297\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1470.858189262634\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4762.405331586078\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14534.206606405469\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19613.620048401404\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28127.881182212528\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28016.834785608116\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27024.350335496558\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22915.14457651853\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9060.625425472746\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8197.478974648828\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3589.0703624093635\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2501.5050578219093\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.3435275393776\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=13, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.44713837031903\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=11, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3141510768801965\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36235\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8516405634495187\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5295443343451148\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5066899443553049\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.36531243889662307\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5902559858482543\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5657419337280813\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.528831456001564\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3675328861747756\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5960814360373416\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.293803205467332\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.32441123253310616\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2933649355281006\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4205118306636893\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.3030489040664283\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2806242795034011\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36291\n",
      "Quantizing model to 16 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36340\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1470.813300863047\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4762.259990073838\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14533.763044083424\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19613.0214699521\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28127.02276120413\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28015.979753570227\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27023.52559260476\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22914.44524046163\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9060.34890868984\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8197.228799809061\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3588.960829331498\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2501.428715616098\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.3122050647066\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=14, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.445293613753876\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=12, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3141109709481038\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36342\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8516665550433491\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5295604957421223\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5067054082491081\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.3653235880280061\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.5902740001309208\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5657591998555832\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.528847595641923\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.36754410307296803\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5960996281094908\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29381217217689276\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.3244211333825395\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2933738888619079\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4205246644496462\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30305815294954086\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.28063284399950994\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36321\n",
      "Quantizing model to 17 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36347\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 1029.26025390625}\n",
      "HW next input range: 1470.7908576906832\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 4360.57763671875}\n",
      "HW next input range: 4762.187322644371\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 8838.3828125}\n",
      "HW next input range: 14533.541273074894\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 12915.873046875}\n",
      "HW next input range: 19612.722194428028\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 15248.2734375}\n",
      "HW next input range: 28126.593570347926\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 17254.228515625}\n",
      "HW next input range: 28015.552257121708\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 18250.271484375}\n",
      "HW next input range: 27023.11324003602\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 13415.96875}\n",
      "HW next input range: 22914.095588439955\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 7997.498046875}\n",
      "HW next input range: 9060.210656627452\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 4699.66796875}\n",
      "HW next input range: 8197.103718115313\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 3049.436279296875}\n",
      "HW next input range: 3588.906065299617\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1789.3045654296875}\n",
      "HW next input range: 2501.3905462605535\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 752.4696655273438}\n",
      "HW next input range: 1026.2965445442967\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=15, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 57.01225662231445}\n",
      "HW next input range: 60.44437127769507\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=13, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3140909189000234\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36341\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8516795508402644\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.529568576440626\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.5067131401960097\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.36532916259369763\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.590283007272254\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.5657678329193342\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5288556654621024\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.3675497115220643\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5961087241455654\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29381665553167313\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19090983271598816\n",
      "HW weight range: 0.3244260838072562\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.20506678521633148\n",
      "HW weight range: 0.2933783655288115\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.28678205609321594\n",
      "HW weight range: 0.4205310813426246\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.1606535166501999\n",
      "HW weight range: 0.30306277739109716\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.1843913495540619\n",
      "HW weight range: 0.2806371262475644\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/OF2_P1_008_val0.3397_025_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 340 files belonging to 17 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 10\n",
      "Precise test accuracy: 0.93824\n",
      "Precise test loss: 0.36349\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.19411765038967133,\n",
      "        \"loss\": 2.4886764947105857\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.8823529481887817,\n",
      "        \"loss\": 0.45196620506398816\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9147058725357056,\n",
      "        \"loss\": 0.3232081560527577\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.929411768913269,\n",
      "        \"loss\": 0.3567375638905694\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.34921999117907354\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9411764740943909,\n",
      "        \"loss\": 0.35332657239016363\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3603435467271244\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.36255021866630105\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3629711620947894\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3634037445573246\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3634724182241103\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.061764705926179886,\n",
      "        \"loss\": 2.9615096905652214\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.7029411792755127,\n",
      "        \"loss\": 1.1030293156118953\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9117646813392639,\n",
      "        \"loss\": 0.34907535244436827\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9264705777168274,\n",
      "        \"loss\": 0.35243354509858527\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3425539942348705\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9411764740943909,\n",
      "        \"loss\": 0.35442800101111915\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.35851628079133874\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3613246321678162\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3623474296401529\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3634230136871338\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.36341140200110045\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.2970588207244873,\n",
      "        \"loss\": 2.2686318285324996\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.8852941393852234,\n",
      "        \"loss\": 0.4520495968706468\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9205882549285889,\n",
      "        \"loss\": 0.3310048487256555\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9323529601097107,\n",
      "        \"loss\": 0.34049167352564197\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9411764740943909,\n",
      "        \"loss\": 0.35277153183432186\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3565073363921222\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.36084365003249225\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3621959798476275\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3629122166072621\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.3632099677534664\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9382352828979492,\n",
      "        \"loss\": 0.36349259614944457\n",
      "    }\n",
      "}\n",
      "CPU times: user 3min 6s, sys: 9.63 s, total: 3min 16s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quant_bw_search(model, model_name, range(7,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82670f3c",
   "metadata": {
    "_cell_guid": "b46cc0ab-15e2-4593-8e54-9530b0f909c3",
    "_uuid": "8dbc6ebe-a4aa-4de1-a399-5930df25ea63",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023445,
     "end_time": "2025-09-23T11:14:49.993173",
     "exception": false,
     "start_time": "2025-09-23T11:14:49.969728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7653335,
     "sourceId": 12154317,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 376060,
     "modelInstanceId": 354740,
     "sourceId": 434975,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 376118,
     "modelInstanceId": 354800,
     "sourceId": 435045,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 225.385463,
   "end_time": "2025-09-23T11:14:53.042820",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T11:11:07.657357",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
