{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffe7432",
   "metadata": {
    "papermill": {
     "duration": 0.003197,
     "end_time": "2025-09-23T11:11:50.576502",
     "exception": false,
     "start_time": "2025-09-23T11:11:50.573305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f5960",
   "metadata": {
    "papermill": {
     "duration": 0.002191,
     "end_time": "2025-09-23T11:11:50.581296",
     "exception": false,
     "start_time": "2025-09-23T11:11:50.579105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38b449c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:50.587049Z",
     "iopub.status.busy": "2025-09-23T11:11:50.586784Z",
     "iopub.status.idle": "2025-09-23T11:11:56.945031Z",
     "shell.execute_reply": "2025-09-23T11:11:56.944316Z"
    },
    "papermill": {
     "duration": 6.362894,
     "end_time": "2025-09-23T11:11:56.946504",
     "exception": false,
     "start_time": "2025-09-23T11:11:50.583610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aris-gk3/ml_project_util.git\r\n",
      "  Cloning https://github.com/aris-gk3/ml_project_util.git to /tmp/pip-req-build-1ru_nwnl\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aris-gk3/ml_project_util.git /tmp/pip-req-build-1ru_nwnl\r\n",
      "  Resolved https://github.com/aris-gk3/ml_project_util.git to commit 62c6be5dba2d44b545daa677b6e6d9dcd247920b\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: ml_project_util\r\n",
      "  Building wheel for ml_project_util (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for ml_project_util: filename=ml_project_util-0.1-py3-none-any.whl size=23354 sha256=b0bbaac82f606a881ca432e8740b5c09932d73df48b19db6e49218672d03d9b3\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7_ignni9/wheels/9b/33/7a/e8e8f55a4c6aa39df26369c48b9e3497c6dde4c7663912f8ef\r\n",
      "Successfully built ml_project_util\r\n",
      "Installing collected packages: ml_project_util\r\n",
      "Successfully installed ml_project_util-0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aris-gk3/ml_project_util.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccfdd15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:11:56.953199Z",
     "iopub.status.busy": "2025-09-23T11:11:56.952935Z",
     "iopub.status.idle": "2025-09-23T11:12:12.814246Z",
     "shell.execute_reply": "2025-09-23T11:12:12.813647Z"
    },
    "papermill": {
     "duration": 15.866136,
     "end_time": "2025-09-23T11:12:12.815604",
     "exception": false,
     "start_time": "2025-09-23T11:11:56.949468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 11:11:58.511492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758625918.732486      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758625918.791114      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Local imports\n",
    "from ml_project_util.path import path_definition\n",
    "from ml_project_util.flatten_model import flatten_condtitional\n",
    "from ml_project_util.quantization_util import quant_weights, quant_activations, quant_model, quant_bw_search, model_evaluation_precise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc73120",
   "metadata": {
    "papermill": {
     "duration": 0.002536,
     "end_time": "2025-09-23T11:12:12.821267",
     "exception": false,
     "start_time": "2025-09-23T11:12:12.818731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Variable Paths, Execution Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a83e0a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:12.827497Z",
     "iopub.status.busy": "2025-09-23T11:12:12.827112Z",
     "iopub.status.idle": "2025-09-23T11:12:12.872646Z",
     "shell.execute_reply": "2025-09-23T11:12:12.872022Z"
    },
    "papermill": {
     "duration": 0.049836,
     "end_time": "2025-09-23T11:12:12.873689",
     "exception": false,
     "start_time": "2025-09-23T11:12:12.823853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_rel_path set to: uc-merced-21-restructured-and-converted\n"
     ]
    }
   ],
   "source": [
    "dict = path_definition(ds_rel_path='uc-merced-21-restructured-and-converted')\n",
    "# /kaggle/input/uc1_p1_ft/keras/default/1/UC1_P1_FT2_015_val0.1273.keras\n",
    "dict = path_definition()\n",
    "BASE_PATH = dict['BASE_PATH']\n",
    "PATH_DATASET = dict['PATH_DATASET']\n",
    "PATH_TEST = dict['PATH_TEST']\n",
    "PATH_RAWDATA = dict['PATH_RAWDATA']\n",
    "PATH_JOINEDDATA = dict['PATH_JOINEDDATA']\n",
    "PATH_SAVEDMODELS = dict['PATH_SAVEDMODELS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f91314",
   "metadata": {
    "papermill": {
     "duration": 0.002494,
     "end_time": "2025-09-23T11:12:12.878947",
     "exception": false,
     "start_time": "2025-09-23T11:12:12.876453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed414fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:12.885020Z",
     "iopub.status.busy": "2025-09-23T11:12:12.884609Z",
     "iopub.status.idle": "2025-09-23T11:12:17.653005Z",
     "shell.execute_reply": "2025-09-23T11:12:17.652396Z"
    },
    "papermill": {
     "duration": 4.772901,
     "end_time": "2025-09-23T11:12:17.654391",
     "exception": false,
     "start_time": "2025-09-23T11:12:12.881490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758625934.228575      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1758625934.229271      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Two first letters for dataset, next letter stands for parameter\n",
    "# 3 numbers are for the number of epochs, and the last 4 digits are for validation loss\n",
    "# e.g. CD4_P2_FT_003_val0.0336\n",
    "model_name = 'UC1_P1_FT2_015_val0.1273'\n",
    "# parent_folder = model_name[:3]\n",
    "# filepath = f'{PATH_SAVEDMODELS}/{parent_folder}/{model_name}.keras'\n",
    "filepath = '/kaggle/input/uc1_p1_ft/keras/default/1/UC1_P1_FT2_015_val0.1273.keras'\n",
    "model = tf.keras.models.load_model(filepath)\n",
    "model = flatten_condtitional(model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd3b87",
   "metadata": {
    "papermill": {
     "duration": 0.002629,
     "end_time": "2025-09-23T11:12:17.660159",
     "exception": false,
     "start_time": "2025-09-23T11:12:17.657530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Quantize & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dead9f20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:17.666443Z",
     "iopub.status.busy": "2025-09-23T11:12:17.666034Z",
     "iopub.status.idle": "2025-09-23T11:12:25.823417Z",
     "shell.execute_reply": "2025-09-23T11:12:25.822682Z"
    },
    "papermill": {
     "duration": 8.161802,
     "end_time": "2025-09-23T11:12:25.824594",
     "exception": false,
     "start_time": "2025-09-23T11:12:17.662792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758625938.268765      19 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Number: 13\n",
      "Precise test accuracy: 0.96190\n",
      "Precise test loss: 0.12592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.96190476, 0.12592056242394306)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation_precise(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef49c862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:25.832858Z",
     "iopub.status.busy": "2025-09-23T11:12:25.832623Z",
     "iopub.status.idle": "2025-09-23T11:12:25.835644Z",
     "shell.execute_reply": "2025-09-23T11:12:25.835124Z"
    },
    "papermill": {
     "duration": 0.008143,
     "end_time": "2025-09-23T11:12:25.836583",
     "exception": false,
     "start_time": "2025-09-23T11:12:25.828440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qw_model = quant_weights(model, model_name, num_bits=8, mode='quant')\n",
    "# qwa_model =  quant_activations(model, model_name, num_bits=8, mode='eval', design='hw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5977f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:25.844460Z",
     "iopub.status.busy": "2025-09-23T11:12:25.844252Z",
     "iopub.status.idle": "2025-09-23T11:12:25.847025Z",
     "shell.execute_reply": "2025-09-23T11:12:25.846560Z"
    },
    "papermill": {
     "duration": 0.007949,
     "end_time": "2025-09-23T11:12:25.848068",
     "exception": false,
     "start_time": "2025-09-23T11:12:25.840119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qwa_model =  quant_model(model, model_name, num_bits=8, mode='eval', design='hw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc3e108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:25.855713Z",
     "iopub.status.busy": "2025-09-23T11:12:25.855504Z",
     "iopub.status.idle": "2025-09-23T11:12:25.858526Z",
     "shell.execute_reply": "2025-09-23T11:12:25.858003Z"
    },
    "papermill": {
     "duration": 0.007884,
     "end_time": "2025-09-23T11:12:25.859616",
     "exception": false,
     "start_time": "2025-09-23T11:12:25.851732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# qwa_model = quant_bw_search(model, model_name, range(7,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af608508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:12:25.867299Z",
     "iopub.status.busy": "2025-09-23T11:12:25.867072Z",
     "iopub.status.idle": "2025-09-23T11:21:52.380591Z",
     "shell.execute_reply": "2025-09-23T11:21:52.379812Z"
    },
    "papermill": {
     "duration": 566.519437,
     "end_time": "2025-09-23T11:21:52.382610",
     "exception": false,
     "start_time": "2025-09-23T11:12:25.863173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing model to 7 bits...\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "Saved activation ranges in /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Saved json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.13333\n",
      "Precise test loss: 2.88758\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1494.113945603282\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=10, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5173.92582681101\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 16237.410050891694\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=9, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 19214.083039828\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31979.00942030549\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 36105.41051764266\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30491.342341466083\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12534.64398294025\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7754.364245844384\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6110.739416052188\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=8, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2715.3174096331118\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 790.9527463905794\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=7, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 442.23613880259086\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=6, N_i=13\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 77.56400876851856\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=3, N_i=10\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3635263051184001\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.04762\n",
      "Precise test loss: 3.49324\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.8966528810949649\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5360693728538056\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.43738697455485287\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.41736653753431\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6586469932414699\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4876110261795617\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5047803950299872\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5660853392505871\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5111143947241417\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29364594334165095\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2664906279927513\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39331330543998116\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46065295674695794\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27398977869511165\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3192610604965272\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.11190\n",
      "Precise test loss: 2.99673\n",
      "Quantizing model to 8 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.60238\n",
      "Precise test loss: 1.25305\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'8b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=11, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1482.3492688662482\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=11, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5133.186253371553\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=10, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 16109.556428443728\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=10, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 19062.791047388408\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31727.20619652356\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35821.1159466376\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30251.253031690758\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12435.94599882261\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7693.306259656633\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6062.623357658076\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=9, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2693.9369575887567\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 784.7247720095512\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=8, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 438.7539644813106\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=7, N_i=15\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.95326854199479\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=4, N_i=12\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3527898775190426\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.34048\n",
      "Precise test loss: 2.28431\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'8b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9037691738020678\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5403238916859787\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4408582997497326\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4206789703718839\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.663874350330688\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49148095495876454\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5087865886413363\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5705780800382901\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5151708581743332\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29597646670150535\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2686056329768208\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39643483961013976\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.464308932594156\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27616430074824744\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3217948784369758\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.62857\n",
      "Precise test loss: 1.21417\n",
      "Quantizing model to 9 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.89762\n",
      "Precise test loss: 0.31204\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1476.5361344785374\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=12, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5113.056111201468\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 16046.381697351791\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=11, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18988.035004065317\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31602.785780066602\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35680.64098214098\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30132.6206668606\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12387.177583140952\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7663.136431187391\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6038.848364098633\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=10, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2683.372498931546\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 781.6474199624549\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=9, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 437.033360699031\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=8, N_i=17\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.65149101830069\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=5, N_i=14\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3474848191758306\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.80952\n",
      "Precise test loss: 0.57822\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'9b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9073273201556192\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5424511511020652\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4425939623471725\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42233518679067084\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6664880288752969\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.493415919348366\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5107896854470108\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5728244504321417\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.517199089899429\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29714172838143255\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2696631354688555\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39799560669521905\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.466136920517755\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27725156177481536\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3230617874072001\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.90476\n",
      "Precise test loss: 0.30647\n",
      "Quantizing model to 10 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.92857\n",
      "Precise test loss: 0.21168\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1473.6466312799494\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=13, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5103.050130553325\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 16014.979776221946\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=12, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18950.876422844056\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31540.940798109525\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35610.81585301742\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30073.65272035011\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12362.93653111915\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7648.140078093091\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6027.030656928186\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=11, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2678.121280734028\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 780.1177772619413\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=10, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 436.17810950392527\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=9, N_i=19\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.50148810045665\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=6, N_i=16\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3448478625825318\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.92143\n",
      "Precise test loss: 0.23800\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'10b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.909106393332395\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5435147808101085\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44346179364589244\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4231632950000643\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6677948681476015\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4943834015431667\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.511791233849848\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5739476356290675\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5182132057619769\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2977243592213961\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2701918867148729\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39877599023775867\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46705091447955455\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2777951922880993\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32369524189231225\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.20304\n",
      "Quantizing model to 11 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19355\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1472.2061164888644\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=14, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5098.06181175513\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15999.324859529646\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=13, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18932.35161695662\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31510.10898892271\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35576.005671342915\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30044.25521035954\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12350.851549172798\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7640.66389033347\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6021.139131359341\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=12, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2675.5033713686967\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 779.3551987895445\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=11, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.7517379403828\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=10, N_i=21\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.42670658716199\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=7, N_i=18\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3435332507911508\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19674\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'11b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9099959299207828\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.54404659566413\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44389570929525246\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.423577349104761\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6684482877837536\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4948671426405671\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5122920080512667\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5745092282275304\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5187202636932509\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2980156746413779\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27045626233788156\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3991661820090285\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46750791146045434\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2780670075447413\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32401196913486835\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19532\n",
      "Quantizing model to 12 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19384\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1471.486914673286\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=15, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5095.571307694673\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15991.508872788303\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=14, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18923.10278861419\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31494.7156772525\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35558.62608869936\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30029.57799726215\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12344.817913828796\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7636.931274852114\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6018.197685765125\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=13, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2674.196335036812\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.9744683553532\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=12, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.53886459502843\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=11, N_i=23\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.38937062888785\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=8, N_i=20\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.342876908216265\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19226\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'12b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9104406982149768\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5443125030911409\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4441126671199324\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4237843761571094\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6687749976018298\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49510901318926726\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5125423951519761\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5747900245267618\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5189737926588879\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2981613323513688\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27058845014938593\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3993612778946634\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4677364099509042\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2782029151730623\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3241703327561464\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19381\n",
      "Quantizing model to 13 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93095\n",
      "Precise test loss: 0.19573\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1471.1275772093854\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=16, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5094.326967936994\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15987.603742416437\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=15, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18918.481762292184\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31487.024659993098\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35549.94266352508\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30022.24476698199\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12341.803306279631\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7635.066334369855\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6016.728040420616\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=14, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2673.5432956387563\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.7842425999551\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=13, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.43250589793564\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=12, N_i=25\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.37071632592597\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=9, N_i=22\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3425489773473478\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93571\n",
      "Precise test loss: 0.19478\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'13b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9106630823620737\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5444454568046463\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4442211460322724\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4238878896832836\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6689383525108679\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49522994846361734\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5126675887023308\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5749304226763775\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5191005571417063\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29823416120636426\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27065454405513806\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3994588258374809\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4678506591961291\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27827086898722275\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3242495145667854\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93095\n",
      "Precise test loss: 0.19569\n",
      "Quantizing model to 14 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93095\n",
      "Precise test loss: 0.19591\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.9479742821225\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=17, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.705025931386\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15985.651892368529\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=16, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18916.17209536967\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31483.180559802648\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35545.602541114684\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30018.579494760408\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12340.296554563567\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7634.134205651217\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.993486881314\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=15, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2673.216895529412\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.6891645578845\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=14, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3793460266259\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=13, N_i=27\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.36139259056692\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=10, N_i=24\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3423850719661554\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19582\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'14b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9107742744356222\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.544511933661399\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4442753854884424\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42393964644637067\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690200299653869\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4952904161007924\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.512730185477508\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750006217511854\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5191639393831156\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.298270575633862\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27068759100801415\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995075998088896\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679077838187416\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.278304845894303\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32428910547210493\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93095\n",
      "Precise test loss: 0.19659\n",
      "Quantizing model to 15 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19759\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.858189262634\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=18, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.394111872549\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15984.676146052692\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=17, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18915.017473377648\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31481.258861666785\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35543.43287728382\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30016.74719423579\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12339.543316661195\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.6682266360385\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.626277366154\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=16, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2673.0537253593866\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.6416342420354\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=15, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3527709581997\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=14, N_i=29\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35673157655297\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=11, N_i=26\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.342303134282461\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19717\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'15b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108298704723964\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445451720897754\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443025052165274\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239655248279142\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690608686926465\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953206499193799\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127614838650967\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750357212885893\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5191956305038202\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29828878284761084\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707041144844522\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39953198679459395\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46793634613004786\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27832183434784313\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32430890092476466\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19738\n",
      "Quantizing model to 16 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19726\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.813300863047\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=19, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.23866907608\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15984.188317562257\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=18, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18914.440215237646\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31480.2981005699\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35542.348144690746\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30015.831127852107\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12339.166732191556\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.435258459927\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.442689418604\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=17, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.9721477439393\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.6178712599423\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=16, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.33948464053384\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=15, N_i=31\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3544012829168\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=12, N_i=28\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422621691915377\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19705\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'16b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108576684907835\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445617913039635\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443160650805699\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.423978464018686\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690812880562762\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49533576682867364\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.512777133058891\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750532710572912\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192114760641725\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2982978864544853\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27071237622267125\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39954418028744615\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46795062728570097\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783303285746132\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32431879865109453\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19755\n",
      "Quantizing model to 17 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19773\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7908576906832\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=20, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.1609512357045\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.94441448272\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=19, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18914.15159938024\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.81774201187\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.805803222145\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30015.373115627677\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.978448576203\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.318779704172\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.350899646887\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=18, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.9313608034076\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.6059903127956\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=17, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3328417858052\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=16, N_i=33\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3532361894357\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=13, N_i=30\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422416875837069\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19758\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'17b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108715674999771\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445701009110576\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44432284501259117\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239849336140719\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690914977380911\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49534332528332053\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127849576557882\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750620459416422\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192193988443486\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2983024382579225\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27071650709178074\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955027703387225\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679577678635275\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833457568799824\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243237475142595\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19769\n",
      "Quantizing model to 18 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19774\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'18b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=21, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.779636361345\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=21, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.122093204933\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=20, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.82246573422\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=20, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18914.007294754505\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.57756823016\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.534638694495\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30015.14411475704\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.884308923276\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.260541659298\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.305005811488\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=19, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.910967799915\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.6000499751899\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=18, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3295204344629\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=17, N_i=35\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3526536560287\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=14, N_i=32\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422314470141867\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19767\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'18b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108785170045739\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445742557146047\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443262349786018\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42398816841176484\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690966025789985\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49534710451064395\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127888699542368\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750664333838177\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192233602344367\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2983047141596411\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27071857252633547\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955332540708527\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679613381524408\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833669924469073\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.324326221945842\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19778\n",
      "Quantizing model to 19 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19779\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'19b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=22, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.774025760885\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=22, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.102664411896\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=21, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.76149205777\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=21, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.93514326736\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.4574827136\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.39905798229\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30015.029615632076\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.837239635488\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.231422970102\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.282059156396\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=20, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.9007714148584\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5970798403781\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=19, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.32785977779673\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=18, N_i=37\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3523623926585\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=15, N_i=34\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.342226326788024\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19772\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'19b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108819917568722\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445763331163782\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443279299616071\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239897858106113\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6690991549994523\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953489941243057\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.512790826103461\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750686271049055\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192253409294808\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2983058521105004\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707196052436129\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995548495936918\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46796312329689743\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.278337761023037\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243274591616332\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19780\n",
      "Quantizing model to 20 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19783\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'20b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=23, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7712204767072\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=23, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.092950070963\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=22, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.731005393993\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=22, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.899067730214\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.397440298886\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.331268014095\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.97236639718\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.81370512626\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.216863708814\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.270585894501\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=21, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8956732515026\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5955947814698\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=20, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3270294542149\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=19, N_i=39\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35221676180672\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=16, N_i=36\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422237666895915\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19778\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'20b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108837291330215\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.544577371817265\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443287774531098\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239905945100345\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691004312096791\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49534993893113655\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127918041780732\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750697239654493\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192263312770028\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830642108593003\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072012160225156\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955561168699505\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679640158691257\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783382919122101\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243280777695288\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19783\n",
      "Quantizing model to 21 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19784\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'21b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=24, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7698178386313\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=24, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.088092914393\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=23, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.715762105714\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=23, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.88103001325\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.367419177423\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.29737312697\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.94374186163\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.801937905311\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.209584098998\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.264849279966\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=22, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8931241771174\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.59485225414\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=21, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3266142936117\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=20, N_i=41\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35214394658914\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=17, N_i=38\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422224866440375\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19779\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'21b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108845978210961\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445778911677084\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443292011988611\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399099885974617\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691010693147925\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.495350411334552\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127922932153793\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750702723957213\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192268264507638\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830670557364486\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707203797815709\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995559927336467\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679644621552399\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783385573567967\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243283870734766\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19784\n",
      "Quantizing model to 22 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19784\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'22b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=25, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7691165205965\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=25, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.085664339583\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=24, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.70814047248\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=24, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.872011167667\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.352408638162\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.28042570765\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.929429614327\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.796054303255\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.205944299296\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.261980976801\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=23, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.891849641748\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5944809910063\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=22, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.32640671360707\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=21, N_i=43\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35210753903243\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=18, N_i=40\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422218466221763\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19780\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'22b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108850321651334\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.54457815084293\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44432941307173673\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399120103460197\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691013883673492\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953506475362597\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127925377340323\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750705466108572\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192270740376443\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2983068478175023\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072050887123056\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995561832569725\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.467964685298297\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833869007908996\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243285417254505\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 23 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'23b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=26, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.76876586183\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=26, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.084450053046\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=25, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.704329658587\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=25, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.867501748104\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.3449033739\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.27195200405\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.922273495795\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.79311250433\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.204124400747\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.260546826245\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=24, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8912123745195\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5942953595722\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=23, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.326302923679\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=22, N_i=45\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35208933526711\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=19, N_i=42\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422215266114743\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19780\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'23b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.910885249337152\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445782806805409\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443295190081746\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399130212202985\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691015478936276\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49535076563711355\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127926599933589\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750706837184253\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192271978310846\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.298306918939431\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072057341606043\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995562785186354\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679647968698255\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783387564402366\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243286190514375\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 24 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'24b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=27, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7685905325095\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=27, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083842909994\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=26, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.702424252322\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=26, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.865247039124\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.341150743112\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.267715153765\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.91869543781\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.791641605394\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.203214451798\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259829751223\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=25, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8908937410188\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5942025438884\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=24, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3262510287335\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=23, N_i=47\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3520802333877\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=20, N_i=44\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422213666061806\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19780\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'24b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108853579231614\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445783455993463\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443295719763935\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399135266574384\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691016276567667\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953508246875405\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927211230221\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750707522722093\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192272597278047\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830695450039535\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072060568847534\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995563261494669\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679648526555898\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833878962080993\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.324328657714431\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 25 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'25b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=28, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7685028678648\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=28, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083539338523\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=27, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.70147154936\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=27, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.86411968484\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.339274428054\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.265596729005\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.916906409137\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790906156057\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.2027594774045\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259471213777\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=26, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.890734424297\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941561360547\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=25, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3262250812654\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=24, N_i=49\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.3520756824488\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=21, N_i=46\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422212866035481\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'25b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.910885412216166\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.544578378058749\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44432959846050296\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239913779376008\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691016675383363\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49535085421275393\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927516878538\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750707865491013\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192272906761648\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830697228087755\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072062182468276\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995563499648826\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46796488054847196\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783388062110966\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243286770459277\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 26 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'26b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=29, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7684590355464\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=29, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083387552801\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=28, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700995197922\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=28, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.863556007746\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.338336270608\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.26453751672\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.91601189488\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790538431422\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.202531990229\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259291945069\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=27, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8906547659435\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941329321399\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=26, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3262121075325\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=25, N_i=51\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35207340697956\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=22, N_i=48\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422212466022354\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'26b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854393626683\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445783942884503\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443296117025577\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239913905735293\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691016874791211\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953508689753607\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927669702695\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708036875473\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273061503447\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830698117111865\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707206298927865\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955636187259047\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.467964894494913\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833881450623993\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243286867116761\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 27 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'27b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=30, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7684371193882\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=30, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083311659943\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=29, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700757022214\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=29, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.863274169213\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.337867191905\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.2640079106\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.91556463777\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790354569113\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.202418246645\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.25920231072\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=28, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8906149367685\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941213301832\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=27, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.32620562066637\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=26, N_i=53\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.352072269245\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=23, N_i=50\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.34222122660158\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'27b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854529359195\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.544578402403301\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.44432961832358503\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239913968914935\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691016974495135\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.495350876356664\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927746114774\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708122567703\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273138874348\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830698561623914\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.27072063392683837\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995563678264444\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679649014681335\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783388186538116\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32432869154455024\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 28 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'28b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=31, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7684261613094\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=31, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083273713515\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=30, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700637934362\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=30, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.86313324995\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.337632652558\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.26374310754\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.915341009222\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.79026263796\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.202361374855\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259157493546\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=29, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8905950221815\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941155292048\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=28, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.32620237723336\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=27, N_i=55\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35207170037772\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=24, N_i=52\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422212166012526\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'28b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854597225451\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445784064607264\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443296216340987\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399140005047564\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691017024347098\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.49535088004731576\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927784320814\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708165413817\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273177559799\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830698783879944\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707206359438643\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955637080337136\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679649049547438\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783388207275974\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32432869396098735\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 29 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'29b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=32, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.76842068227\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=32, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.0832547403015\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=31, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700578390437\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=31, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.86306279032\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.33751538289\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.26361070602\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.915229194947\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790216672383\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.20233293896\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259135084959\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=30, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.890585064888\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941126287157\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=29, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3262007555169\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=28, N_i=57\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35207141594408\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=25, N_i=54\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.3422212116010888\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'29b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854631158578\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445784084894391\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443296232893556\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.42399140162996674\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691017049273078\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953508818926416\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927803423834\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708186836875\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273196902523\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830698895007957\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707206369523773\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995563722918348\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.46796490669804897\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.2783388217644903\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32432869516920587\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 30 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'30b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=33, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7684179427504\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=33, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083245253694\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=32, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700548618473\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=32, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.863027560503\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.337456748053\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.26354450525\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.915173287813\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790193689596\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.2023187210125\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259123880666\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=31, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.8905800862412\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941111784712\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=30, N_i=60\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.32619994465864\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=29, N_i=59\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35207127372728\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=26, N_i=56\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.342221209101007\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'30b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854648125143\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445784095037954\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.444329624116984\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.4239914024197122\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691017061736069\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.4953508828153045\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927812975344\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708197548403\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273206573885\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.29830698950571966\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.2707206374566338\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.3995563730360666\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679649075697015\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833882228293677\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.32432869577331513\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Quantizing model to 31 bits...\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Quantization on arbitrary symmetric ranges is applied.\n",
      "Read sw activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "'31b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "Read input range json from /kaggle/working/Docs_Reports/Quant/Ranges/input_range.json\n",
      "For layer 1.\n",
      "For layer block1_conv1: k=34, N_i=65\n",
      "Next input range: {'min': 0.0, 'max': 1100.794189453125}\n",
      "HW next input range: 1470.7684165729904\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "For layer block1_conv2: k=34, N_i=65\n",
      "Next input range: {'min': 0.0, 'max': 4795.75}\n",
      "HW next input range: 5093.083240510391\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "For layer block2_conv1: k=33, N_i=64\n",
      "Next input range: {'min': 0.0, 'max': 8523.5751953125}\n",
      "HW next input range: 15983.700533732494\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "For layer block2_conv2: k=33, N_i=64\n",
      "Next input range: {'min': 0.0, 'max': 14455.6904296875}\n",
      "HW next input range: 18913.863009945595\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "For layer block3_conv1: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 19344.654296875}\n",
      "HW next input range: 31479.337427430633\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "For layer block3_conv2: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 19164.783203125}\n",
      "HW next input range: 35541.263511404875\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "For layer block3_conv3: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 19655.125}\n",
      "HW next input range: 30014.915145334242\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "For layer block4_conv1: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 11303.0888671875}\n",
      "HW next input range: 12338.790182198203\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "For layer block4_conv2: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 5868.87255859375}\n",
      "HW next input range: 7633.202311612039\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "For layer block4_conv3: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 3501.451416015625}\n",
      "HW next input range: 6015.259118278519\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "For layer block5_conv1: k=32, N_i=63\n",
      "Next input range: {'min': 0.0, 'max': 1895.830322265625}\n",
      "HW next input range: 2672.890577596918\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "For layer block5_conv2: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 757.4910888671875}\n",
      "HW next input range: 778.5941104533489\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "For layer block5_conv3: k=31, N_i=62\n",
      "Next input range: {'min': 0.0, 'max': 354.479248046875}\n",
      "HW next input range: 435.3261995392295\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "For layer dense: k=30, N_i=61\n",
      "Next input range: {'min': 0.0, 'max': 49.33266830444336}\n",
      "HW next input range: 76.35207120261886\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "For layer dense_1: k=27, N_i=58\n",
      "Next input range: {'min': 0.0, 'max': 1.0}\n",
      "HW next input range: 1.342221207850966\n",
      "\n",
      "\n",
      "Saved activation_hw_range_dict json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Weight focused solution chosen.\n",
      "Read hww activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_hw_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19781\n",
      "Read activation range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json\n",
      "Read weight range json from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_range.json\n",
      "'31b' is missing or empty from dictionary.\n",
      "['input_layer', 'block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'dense', 'dense_1']\n",
      "\n",
      "\n",
      "For layer 1.\n",
      "Weight range: 0.6085159182548523\n",
      "HW weight range: 0.9108854656608425\n",
      "\n",
      "\n",
      "For layer 2.\n",
      "Weight range: 0.2891709506511688\n",
      "HW weight range: 0.5445784100109735\n",
      "\n",
      "\n",
      "For layer 3.\n",
      "Weight range: 0.41661107540130615\n",
      "HW weight range: 0.4443296245307983\n",
      "\n",
      "\n",
      "For layer 4.\n",
      "Weight range: 0.277375727891922\n",
      "HW weight range: 0.423991402814585\n",
      "\n",
      "\n",
      "For layer 5.\n",
      "Weight range: 0.5444108247756958\n",
      "HW weight range: 0.6691017067967564\n",
      "\n",
      "\n",
      "For layer 6.\n",
      "Weight range: 0.45931634306907654\n",
      "HW weight range: 0.495350883276636\n",
      "\n",
      "\n",
      "For layer 7.\n",
      "Weight range: 0.3915373682975769\n",
      "HW weight range: 0.5127927817751099\n",
      "\n",
      "\n",
      "For layer 8.\n",
      "Weight range: 0.3138822615146637\n",
      "HW weight range: 0.5750708202904168\n",
      "\n",
      "\n",
      "For layer 9.\n",
      "Weight range: 0.337660014629364\n",
      "HW weight range: 0.5192273211409567\n",
      "\n",
      "\n",
      "For layer 10.\n",
      "Weight range: 0.2562357187271118\n",
      "HW weight range: 0.2983069897835397\n",
      "\n",
      "\n",
      "For layer 11.\n",
      "Weight range: 0.19084161520004272\n",
      "HW weight range: 0.270720637708762\n",
      "\n",
      "\n",
      "For layer 12.\n",
      "Weight range: 0.2053438276052475\n",
      "HW weight range: 0.39955637340818245\n",
      "\n",
      "\n",
      "For layer 13.\n",
      "Weight range: 0.2873474061489105\n",
      "HW weight range: 0.4679649080055278\n",
      "\n",
      "\n",
      "For layer 14.\n",
      "Weight range: 0.21539221704006195\n",
      "HW weight range: 0.27833882254215997\n",
      "\n",
      "\n",
      "For layer 15.\n",
      "Weight range: 0.21766042709350586\n",
      "HW weight range: 0.3243286960753698\n",
      "\n",
      "\n",
      "Saved weight HW range dictionary json in: /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_wt_hw_range.json\n",
      "Quantization on symmetric ranges that enable shifting on interlayer scaling is applied.\n",
      "Activation focused solution chosen.\n",
      "Read hwa activation quantization range from /kaggle/working/Docs_Reports/Quant/Ranges/UC1_P1_FT2_015_activation_range.json.\n",
      "New model input shape: (None, 224, 224, 3)\n",
      "Found 420 files belonging to 21 classes.\n",
      "Start evaluating batches\n",
      "Batch Number: 13\n",
      "Precise test accuracy: 0.93333\n",
      "Precise test loss: 0.19785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.13333334028720856,\n",
      "        \"loss\": 2.8875808806646437\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.6023809313774109,\n",
      "        \"loss\": 1.253049806186131\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.8976190686225891,\n",
      "        \"loss\": 0.31204331942967006\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9285714030265808,\n",
      "        \"loss\": 0.21168107883561224\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.19354558615457443\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.19383950038325218\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9309523701667786,\n",
      "        \"loss\": 0.1957344724131482\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9309523701667786,\n",
      "        \"loss\": 0.19590914093312764\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19759058136315572\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1972580534716447\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19773276545816942\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19774123446217606\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19779255613684654\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1978346137773423\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1978448748588562\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19784494693435373\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19784846706759363\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785033437822547\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785165953494255\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1978517608273597\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785194515827156\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785173162817954\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785193449684552\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1978518921881914\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785194223125777\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.0476190485060215,\n",
      "        \"loss\": 3.493235431398664\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.3404761850833893,\n",
      "        \"loss\": 2.284310883567447\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.8095238208770752,\n",
      "        \"loss\": 0.5782220857484001\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9214285612106323,\n",
      "        \"loss\": 0.23800495632347607\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19674397426701729\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.1922563145912829\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.19477570901314417\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19582151050368946\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1971664498781874\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19705044103875047\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19758179187774658\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19767471986512344\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19772487637542543\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19778113730606578\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19779393371371995\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19779857570926349\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780239959557852\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780424050986767\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780625984782263\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780660823342347\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780684665200257\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780682710309824\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780696893022173\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780710433565435\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780691377818585\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"7b\": {\n",
      "        \"accuracy\": 0.11190476268529892,\n",
      "        \"loss\": 2.996725189118158\n",
      "    },\n",
      "    \"8b\": {\n",
      "        \"accuracy\": 0.6285714507102966,\n",
      "        \"loss\": 1.2141708691914876\n",
      "    },\n",
      "    \"9b\": {\n",
      "        \"accuracy\": 0.9047619104385376,\n",
      "        \"loss\": 0.3064742312544868\n",
      "    },\n",
      "    \"10b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.20304126657900357\n",
      "    },\n",
      "    \"11b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.1953245645477658\n",
      "    },\n",
      "    \"12b\": {\n",
      "        \"accuracy\": 0.9357143044471741,\n",
      "        \"loss\": 0.19380746586691766\n",
      "    },\n",
      "    \"13b\": {\n",
      "        \"accuracy\": 0.9309523701667786,\n",
      "        \"loss\": 0.19568837856252988\n",
      "    },\n",
      "    \"14b\": {\n",
      "        \"accuracy\": 0.9309523701667786,\n",
      "        \"loss\": 0.19658721057432038\n",
      "    },\n",
      "    \"15b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19737666526011058\n",
      "    },\n",
      "    \"16b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19755332576377052\n",
      "    },\n",
      "    \"17b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19768828036529676\n",
      "    },\n",
      "    \"18b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19777913075827416\n",
      "    },\n",
      "    \"19b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19780072683379762\n",
      "    },\n",
      "    \"20b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19783147307378904\n",
      "    },\n",
      "    \"21b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19784195996111348\n",
      "    },\n",
      "    \"22b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19784616075810932\n",
      "    },\n",
      "    \"23b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19784951772363413\n",
      "    },\n",
      "    \"24b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785098057417644\n",
      "    },\n",
      "    \"25b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785116857716015\n",
      "    },\n",
      "    \"26b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.1978516472769635\n",
      "    },\n",
      "    \"27b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785198849581537\n",
      "    },\n",
      "    \"28b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785193923328603\n",
      "    },\n",
      "    \"29b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785191477054642\n",
      "    },\n",
      "    \"30b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785185050041904\n",
      "    },\n",
      "    \"31b\": {\n",
      "        \"accuracy\": 0.9333333373069763,\n",
      "        \"loss\": 0.19785197527990456\n",
      "    }\n",
      "}\n",
      "CPU times: user 8min 46s, sys: 25.6 s, total: 9min 12s\n",
      "Wall time: 9min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "quant_bw_search(model, model_name, range(7,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3d96d6",
   "metadata": {
    "papermill": {
     "duration": 0.056034,
     "end_time": "2025-09-23T11:21:52.496323",
     "exception": false,
     "start_time": "2025-09-23T11:21:52.440289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a02849d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:21:52.608160Z",
     "iopub.status.busy": "2025-09-23T11:21:52.607597Z",
     "iopub.status.idle": "2025-09-23T11:21:52.611179Z",
     "shell.execute_reply": "2025-09-23T11:21:52.610580Z"
    },
    "papermill": {
     "duration": 0.061001,
     "end_time": "2025-09-23T11:21:52.612304",
     "exception": false,
     "start_time": "2025-09-23T11:21:52.551303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # save accuracy in json in\n",
    "# bitwidth = 8\n",
    "# mode = \"qw\"\n",
    "# short_name = model_name[:-10]\n",
    "# accuracy_path = f'{BASE_PATH}/Docs_Reports/Quant/{short_name}_acc_bw_{mode}.json'\n",
    "# # read json\n",
    "# try:\n",
    "#     with open(accuracy_path, 'r') as file:\n",
    "#         acc_bw_dict = json.load(file)\n",
    "# except:\n",
    "#     print('No accuracy per bitwidth json file found in specified path!')\n",
    "#     acc_bw_dict = {}\n",
    "# # add in dict\n",
    "# acc_bw_dict[f'{bitwidth}b'] = { \"accuracy\": 0.97840, \"loss\": 0.11877}\n",
    "# # write json\n",
    "# with open(accuracy_path, 'w') as f:\n",
    "#     json.dump(acc_bw_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d426c",
   "metadata": {
    "papermill": {
     "duration": 0.056256,
     "end_time": "2025-09-23T11:21:52.724942",
     "exception": false,
     "start_time": "2025-09-23T11:21:52.668686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Save qw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6189a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T11:21:52.838836Z",
     "iopub.status.busy": "2025-09-23T11:21:52.838643Z",
     "iopub.status.idle": "2025-09-23T11:21:52.841569Z",
     "shell.execute_reply": "2025-09-23T11:21:52.841042Z"
    },
    "papermill": {
     "duration": 0.060219,
     "end_time": "2025-09-23T11:21:52.842580",
     "exception": false,
     "start_time": "2025-09-23T11:21:52.782361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # same path + qw subfolder\n",
    "# parent_folder = model_name[:3]\n",
    "# short_name = model_name[:-10]\n",
    "# model_path = f'{PATH_SAVEDMODELS}/{parent_folder}/Quant/{short_name}_qw.keras'\n",
    "# model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7661346,
     "sourceId": 12164432,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 377937,
     "modelInstanceId": 356642,
     "sourceId": 437205,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 609.466623,
   "end_time": "2025-09-23T11:21:56.025413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-23T11:11:46.558790",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
